{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6fd949a-d55c-41d3-aee9-e00924ae60c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Very Important Spark Learning - BY LEARNING this PROGRAM - WE BECOME A DATA ENGINEER (DATA CURATION DEVELOPER & DATA ANALYST)\n",
    "Only the usage of SQL/Declarative Language Programming\n",
    "###Points to consider using SQL rather than DSL\n",
    "1. We can't use pure sql alone for achieving all functionalities.\n",
    "2. We will use SQL/DSL appropriately as and when it is applicable (ie: we don't have to write a spark pipeline only using DSL or only using SQL, we can mix best of both).\n",
    "3. We use DSL in data ingestion/egress mostly, using SQL in ingestion/egress is bit challenging.\n",
    "4. Performance wise usage of SQL or DSL is going to almost same, because any way SQL/DSL internally spark is optimizing the code and convert the code in RDD.\n",
    "5. Anytime we can represent DF to SQL or DF to DF or or SQL to DF or SQL to SQL easily.\n",
    "\n",
    "###Trade off between DF & Temp view or DSL & SQL\n",
    "1. By default spark produces DF(mostly)\n",
    "2. DF(XLS) -> DSL functions\n",
    "3. DF(XLS) to Temp View(Temptable) -> SQL Queries\n",
    "4. DSL/SQL internally RDD transformations & actions\n",
    "###We have to understand, how to represent DF to View & View to DF (natural), DF to DF (natural), View to View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a802c40-2c4d-4303-a298-90f06cabcca0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv([\"/Volumes/we47catalog1/we47db1/we47volume1/custs\",\"/Volumes/we47catalog1/we47db1/we47volume1/custs\"],header=False,inferSchema=True).toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\").select(\"*\")\n",
    "#If i want to further write SQL queries rather than writing DSL, we can create a view on top of the dataframe\n",
    "rawdf1.createOrReplaceTempView(\"rawdfview\")\n",
    "spark.sql(\"select * from rawdfview\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95ecdc0-156a-4005-81c0-591a5a59f827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from rawdfview limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afebaeb2-bd26-4d86-9680-d98ebba94f94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "--For extraction/load operations with multiple features, preferably use DSL and not SQL\n",
    "--For initial dataset creation or for ingestion/egress DSL way of creating dataframe is better, because not all the options are supported in SQL ingestion\n",
    "CREATE OR REPLACE TEMPORARY VIEW rawdf1view\n",
    "(\n",
    "  id INT,\n",
    "  firstname STRING,\n",
    "  lastname STRING,\n",
    "  age INT,\n",
    "  profession STRING\n",
    ")\n",
    "USING CSV\n",
    "OPTIONS (\n",
    "  path \"/Volumes/we47catalog1/we47db1/we47volume1/custs\",\n",
    "  header \"false\",\n",
    "  inferSchema \"false\" \n",
    ");\n",
    "\n",
    "select * from rawdf1view limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1647dcf5-b647-407a-8354-022344247a4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "##--For extraction/load operations with multiple features, preferably use DSL and not SQL\n",
    "##--For initial dataset creation or for ingestion/egress DSL way of creating dataframe is better, because not all the options are supported in SQL ingestion\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW rawdf1view\n",
    "(\n",
    "  id INT,\n",
    "  firstname STRING,\n",
    "  lastname STRING,\n",
    "  age INT,\n",
    "  profession STRING\n",
    ")\n",
    "USING CSV\n",
    "OPTIONS (\n",
    "  path \"/Volumes/we47catalog1/we47db1/we47volume1/custs\",\n",
    "  header \"false\",\n",
    "  inferSchema \"false\" \n",
    ")\"\"\");\n",
    "\n",
    "spark.sql(\"select * from rawdf1view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2bfdf4d-9abf-4717-a2f6-c3c13a03fb16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Fundamentally we need to know 3 things?\n",
    "#1. How to represent DF to View?\n",
    "#2. How to represent View to DF?\n",
    "#3. How to represent View to View?\n",
    "\n",
    "#1. How to represent DF to View?\n",
    "#How to represent dataframe to tempview? So we can write sql rather than dsl for transformation and analytics.\n",
    "rawdf1.select(\"*\").where(\"age between 35 and 65\").show()\n",
    "\n",
    "rawdf1.createOrReplaceTempView(\"view1\")\n",
    "df2=spark.sql(\"select * from view1 where age between 35 and 65 limit 20\")\n",
    "display(df2)\n",
    "\n",
    "#3. How to represent View to View?\n",
    "spark.sql(\"select * from view1 where age between 18 and 65 limit 10\").createOrReplaceTempView(\"view2\")\n",
    "display(spark.sql(\"select * from view2\"))\n",
    "#or\n",
    "spark.sql(\"create or replace temp view view3 as select * from view1 where age between 18 and 65 limit 10\")\n",
    "display(spark.sql(\"select * from view3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87b59906-4dda-481d-9084-837dae902c92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--We can directly create tempview from the source\n",
    "--hence DSL is preferred over SQL because SQL doesn't support all the options like DSL, eg. multiple paths with regular expression\n",
    "CREATE OR REPLACE TEMPORARY VIEW rawdf1view\n",
    "(\n",
    "  id INT,\n",
    "  firstname STRING,\n",
    "  lastname STRING,\n",
    "  age INT,\n",
    "  profession STRING\n",
    ")\n",
    "USING CSV\n",
    "OPTIONS (\n",
    "  path \"/Volumes/we47catalog1/we47db1/we47volume1/custs\",\n",
    "  header \"false\",\n",
    "  inferSchema \"false\" \n",
    ");\n",
    "\n",
    "select * from rawdf1view limit 20;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396cff2c-7e8c-4275-9484-08487e30a55e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768916897094}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--both DSL or SQL is preferred\n",
    "\n",
    "select * from rawdf1view where rand() < 0.1;\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78bfc4a2-1450-4e66-ba25-21e8956d9952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "describe rawdf1view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b16f3f-06b5-40d1-9b99-51ebfa102981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA data functions we can use\n",
    "#We identified few patterns on this data\n",
    "#1. Deduplication of rows and given column(s)\n",
    "#2. Null values ratio across all columns\n",
    "#3. Distribution (Dense) of the data across all number columns\n",
    "#4. Min, Max values\n",
    "#5. StdDeviation - \n",
    "#6. Percentile - Distribution percentage from 0 to 100 in 4 quadrants of 25%\n",
    "rawdf1.createOrReplaceTempView(\"rawdf1view\")\n",
    "print(\"actual count of the data\",\"select count(*) from rawdf1view\")\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.distinct().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given cid column count\",rawdf1.dropDuplicates(['id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "display(rawdf1.describe())\n",
    "display(rawdf1.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad83487f-6fd0-44b1-894a-338caceff3af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "###--Equivalent to dropduplicates or distinct function, DSL is preferred over SQL because DSL is simple and lesser LOC\n",
    "spark.sql(\"select distinct * from rawdf1view\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24f4c52d-5ab1-4060-8d46-84a7c7ebf67f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select count(*) as dedup_of_all_columns from (select distinct * from rawdf1view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb73bff9-ac76-4e2f-9832-99e06c07da86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(1) as dedup_id_column from (select distinct id from rawdf1view where id is not null) as inlineview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "187f520d-f194-4ee0-a882-ba7351809d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Importantly, we are using qualify function (advanced feature) to avoid additional temporary views (refer next cell)\n",
    "--df.dropDuplicates(['id']).count()\n",
    "--Prefer DSL over SQL to reduce LOC\n",
    "select count(*) as dedup_id \n",
    "from (select *,row_number() over(partition by id order By id) as rownum from rawdf1view) inlineview where rownum=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d56763e8-8407-4775-b379-5b689d0fb167",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768922256018}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Rather than adding a column and filtering the row number outside of the inline view, we can just filter inside the inline view itself by using qualify function\n",
    "\n",
    "select count(*) as total_id_count from (select * from rawdf1view qualify row_number() over(partition by id order by id)=1) as\n",
    " inlineview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8c1796-e163-40e9-b5cc-9c6a255a735e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select count(*) as total_id_count from \n",
    "(\n",
    "  select *,row_number() over(partition by id order by id) as rownum from rawdf1view) inline\n",
    "  where rownum=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e06d0a-f815-47cf-919c-8607d3e7e5f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ead0631-ff02-42f0-afbf-2b6753c73ca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e5407f1-9cd6-4f81-92f8-d2ea6b655dd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Validation by doing cleansing\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ShortType,IntegerType\n",
    "#print(rawdf1.schema)\n",
    "struttype1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True)])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/we47catalog1/we47db1/we47volume1/custsmodified\",mode='permissive')\n",
    "print(\"after keeping nulls on the wrong data format\",cleandf1.count())#all rows count\n",
    "display(cleandf1)#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato)\n",
    "#or\n",
    "#method2 - drop malformed rows\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/we47catalog1/we47db1/we47volume1/custsmodified\",mode='dropMalformed')\n",
    "print(\"after cleaning wrong data (type mismatch, column number mismatch)\",len(cleandf1.collect()))\n",
    "display(cleandf1)#We are removing the entire row, where ever data format mismatch is there (throwing away the entire potato)\n",
    "print(cleandf1.count())#count will return the original count of the raw data\n",
    "print(len(cleandf1.collect()))#collect+len will return the dropmalformed count of the raw data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4fd2b46-5803-4b0f-88f6-ed7b8e7a3be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Rejection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb6ccd3-0751-443c-8a3e-1abc7f9cb0af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creating rejection dataset to send to our source system for future fix\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ShortType,IntegerType\n",
    "struttype1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True),StructField(\"corruptedrows\",StringType())])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/we47catalog1/we47db1/we47volume1/custsmodified\",mode='permissive',columnNameOfCorruptRecord=\"corruptedrows\")\n",
    "#Create a reject dataset\n",
    "rejectdf1=cleandf1.where(\"corruptedrows is not null\")\n",
    "display(rejectdf1)\n",
    "rejectdf1.write.csv(\"/Volumes/we47catalog1/we47db1/we47volume1/reject\",mode=\"overwrite\",header=True)\n",
    "retaineddf1=cleandf1.where(\"corruptedrows is null\")\n",
    "display(retaineddf1)\n",
    "print(\"Overall rows in the source data is \",len(cleandf1.collect()))\n",
    "print(\"Rejected rows in the source data is \",len(rejectdf1.collect()))\n",
    "print(\"Clean rows in the source data is \",len(retaineddf1.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6125f595-57b9-48da-81ff-68836d6b3a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW cleandf1 (\n",
    "  id INT,\n",
    "  firstname STRING,\n",
    "  lastname STRING,\n",
    "  age SMALLINT,\n",
    "  profession STRING,\n",
    "  corruptedrows STRING\n",
    ")\n",
    "USING CSV\n",
    "OPTIONS (\n",
    "  path \"/Volumes/we47catalog1/we47db1/we47volume1/custsmodified\",\n",
    "  mode \"PERMISSIVE\",\n",
    "  columnNameOfCorruptRecord \"corruptedrows\",\n",
    "  header \"false\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a1f6410-888d-4e15-85c1-fc1d4c2c5e69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DSL or SQL - anything as per our convenience\n",
    "display(spark.sql(\"select * from cleandf1 where corruptedrows is not null\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1167bbe7-3c83-4842-a66f-f09a48183bac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from cleandf1 where corruptedrows is null\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f15ab60a-e52a-468b-8d01-b3736a90fd86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Cleansing \n",
    "na.drop()<br>\n",
    "It is a process of cleaning/removing/deleting unwanted data\n",
    "Eg. I am purchasing potato from a shop, I am cutting down the debris/rotten portion of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a44847d8-fc7c-46e8-8559-fa97f29b0d09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We already know how to do cleansing applying the strict Structure on method1 and method2\n",
    "#Important na functions we can use to do cleansing\n",
    "cleanseddf=rawdf1.na.drop(how=\"any\")#This function will drop any column in a given row with null otherwise this function returns rows with no null columns - In a scenario of if the source send the Datascience Model features (we shouldn't have any one feature with null value, hence we can use this function)\n",
    "print(\"any one row in the raw df with age null\")\n",
    "display(rawdf1.where(\"age is null\"))\n",
    "print(\"any one row in the cleansed df with age null\")\n",
    "display(cleanseddf.where(\"age is null\"))#any one column contains null will be cleaned\n",
    "cleanseddf=rawdf1.na.drop(how=\"any\",subset=[\"id\",\"age\"])#If we need CDE without nulls (Critical Data Elements/Significant columns) columns\n",
    "print(\"any one row in the cleansed df with id or age null\")\n",
    "display(cleanseddf)\n",
    "cleanseddf=rawdf1.na.drop(how=\"all\",subset=[\"firstname\",\"lastname\"])#4000004,Gretchen,,66,\n",
    "print(\"any one row in the cleansed df with firstname and lastname is null\")\n",
    "print(\"Total rows without firstname and lastname with null values\",len(cleanseddf.collect()))\n",
    "display(cleanseddf)#We are taking this DF further for munging..\n",
    "\n",
    "cleandf=rawdf1.na.drop(how='any',subset=['id','age'])\n",
    "display(cleandf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88492387-505d-46b0-b454-0047aa1016d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--DSL is better to use, we don't have to manage all the columns in the filter with not(), or, and operators\n",
    "--rawdf1.na.drop(how=\"any\",subset=[\"id\",\"age\"])\n",
    "create or replace temp view cleandf_any as\n",
    "select * from cleandf1 \n",
    "where id is not null and age is not null;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f667e7-d28a-4801-b8cf-4fdab3946a9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--rawdf1.na.drop(how=\"all\",subset=[\"firstname\",\"lastname\"])\n",
    "CREATE OR REPLACE TEMP VIEW cleanseddf_all AS\n",
    "select * from cleandf1\n",
    "where not(firstname is null and lastname is null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73236201-bcb9-4265-b0b2-367e3d20f04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from cleandf_any limit 10\"))\n",
    "display(spark.sql(\"select * from cleanseddf_all limit 10\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91553c7-31a3-47bf-8590-f7225d1edd9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Scrubbing \n",
    "na.fill() & na.replace()<br>\n",
    "It is a process of polishing/fine tuning/scrubbing/meaningful conversion the data in a usable format\n",
    "Eg. I am purchasing potato from a shop, I am scrubbing/washing mud/sand portion of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35636212-a5e8-4946-bf0b-35293465508a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scrubbeddf1=cleanseddf.na.fill('not provided',subset=[\"lastname\",\"profession\"])#fill will help us replace nulls with some value\n",
    "#display(scrubbeddf1)\n",
    "find_replace_values_dict1={'Pilot':'Captain','Actor':'Celeberity'}\n",
    "find_replace_values_dict2={'not provided':'NA'}\n",
    "scrubbeddf2=scrubbeddf1.na.replace(find_replace_values_dict1,subset=[\"profession\"])#fill function is helping us find and replace the values\n",
    "scrubbeddf3=scrubbeddf2.na.replace(find_replace_values_dict2,subset=[\"lastname\"])\n",
    "#display(scrubbeddf3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a6dfd9-2edc-4c90-96e3-99217baa292c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--na.fill equivalent cleanseddf.na.fill('not provided',subset=[\"lastname\",\"profession\"])\n",
    "CREATE OR REPLACE TEMP VIEW scrubbeddf1 AS\n",
    "SELECT\n",
    "  id,\n",
    "  firstname,\n",
    "  COALESCE(lastname, 'not provided')  AS lastname,\n",
    "  age,\n",
    "  COALESCE(profession, 'not provided') AS profession\n",
    "FROM cleandf1;\n",
    "\n",
    "--na.replace equivalent scrubbeddf1.na.replace({'Pilot':'Captain','Actor':'Celeberity'})\n",
    "--Writing na.replace is simpler than sql case statement or it depends (if i don't have much idea about python dictionary)\n",
    "--scrubbeddf1.withColumn(\"profession\",when(profession='Pilot','Captain').when(profession='Actor','Celeberity').otherwise(profession))\n",
    "--Writing when in DSL is not familiar than sql case statement, hence prefer SQL case than DSL when\n",
    "CREATE OR REPLACE TEMP VIEW scrubbeddf2 AS\n",
    "SELECT\n",
    "  id,\n",
    "  firstname,\n",
    "  lastname,\n",
    "  age,\n",
    "  CASE\n",
    "    WHEN profession = 'Pilot' THEN 'Captain'\n",
    "    WHEN profession = 'Actor' THEN 'Celeberity'\n",
    "    ELSE profession\n",
    "  END AS profession\n",
    "FROM scrubbeddf1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c42e9948-56e6-436c-a3c3-d2b0bb0f5e3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from scrubbeddf1 limit 10\"))\n",
    "display(spark.sql(\"select * from scrubbeddf2 limit 10\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecbcae20-0dfb-43ff-8ab2-8deaec1c08f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####DeDuplication\n",
    "Removal of duplicate rows/columns based on a priority or non priority\n",
    "distinct & dropDuplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0837d7ed-8b54-4dd4-800a-ba62d01bb617",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(scrubbeddf3.where(\"id in ('4000001')\"))#before row level dedup\n",
    "dedupdf1=scrubbeddf3.distinct()#It will remove the row level duplicates\n",
    "display(dedupdf1.where(\"id in ('4000001')\"))\n",
    "\n",
    "print(\"non prioritized deduplication, just remove the duplicates retaining only the first row\")\n",
    "display(dedupdf1.coalesce(1).where(\"id in ('4000003')\"))#before col level dedup\n",
    "dedupdf2=dedupdf1.coalesce(1).dropDuplicates(subset=[\"id\"])#It will remove the column level duplicates (retaining the first row in the dataframe)\n",
    "display(dedupdf2.where(\"id in ('4000003')\"))\n",
    "print(\"prioritized deduplication based on age\")\n",
    "display(dedupdf1.coalesce(1).where(\"id in ('4000003')\"))#before col level dedup\n",
    "#dedupdf1.coalesce(1).where(\"id in ('4000003')\").orderBy([\"id\",\"age\"],ascending=[True,False]).show(3)\n",
    "dedupdf2=dedupdf1.coalesce(1).orderBy([\"id\",\"age\"],ascending=[True,False]).dropDuplicates(subset=[\"id\"])#It will remove the column level duplicates (retaining the first row in the dataframe)\n",
    "display(dedupdf2.where(\"id in ('4000003')\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29f5de46-5c46-42f8-ac99-78b39edc7916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--show me the duplicate data alone\n",
    "create or replace temp view dedupdf1 as\n",
    "select * from (select *,row_number() over(partition by id order by id,age desc)as rown from scrubbeddf2) inlineview where rown=1;\n",
    "\n",
    "----select * from dedupdf1 where rown>1\n",
    "select * from dedupdf1 where rown=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fbd1806-87d2-4039-95c7-17c55055c759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from dedupdf1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f53872-f4f1-4a74-97a1-69d684504145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view dedupdf1 as\n",
    "select count(*) as dedup_count from (select * from scrubbeddf2 qualify row_number() over(partition by id order by id,age desc)=1)as inlineview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16b56e67-1f7a-4d55-bae6-bd435dde1134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from dedupdf1 limit 10\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7da843a5-c723-410b-ad9e-133c08575cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--dedupdf2=dedupdf1.coalesce(1).orderBy([\"id\",\"age\"],ascending=[True,False])\n",
    "--better to use DSL or SQL? SQL is better because it is easy to read and understand and familiar including qualify function\n",
    "\n",
    "create or replace temp view dedupdf2 as\n",
    "select * from (select *,row_number() over(partition by id order by id,age desc)as dedup from scrubbeddf2) as inlineview1 where dedup=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e38725e-69c5-4d65-9d9d-804dc5010842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from dedupdf2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d678911-700f-4450-9774-f3ad4fb1d3b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,initcap,col\n",
    "#withColumn(\"stringcolumnname to add in the df\",lit('hardcoded')/initcap(col(\"colname\")))\n",
    "standarddf1=dedupdf2.withColumn(\"sourcesystem\",lit(\"Retail\"))#SparkSQL - DSL(FBP)\n",
    "display(standarddf1.limit(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d64e97-8de2-4c61-8877-5ecefe831915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--In standardization DSL or SQL? SQL is better because it is easy to read and understand and familiar, rather than using select, withColumn, withColumnRenamed, drop, lit, col and other functions from function library imported, we can simply use SQL select to achieve all these..\n",
    "create or replace temp view standarddf1 as\n",
    "select *,'Retail' as sourcesystem from dedupdf2\n",
    "where upper(id) =lower(id);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b07965c-b4f6-48df-babd-85cb0b950d34",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768991470936}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from standarddf1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f4cdf21-73c1-4c1b-879e-bb05ab23557c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization2 - Column Uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b04326-66ff-4e49-becb-65d19c233a0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "#Basic Exploration/analysis of the profession column for identifying uniformity challenges\n",
    "#standarddf1.createOrReplaceTempView(\"sqlview\")\n",
    "#display(spark.sql(\"select profession,count(*) from sqlview group by profession order by profession\"))#SQL\n",
    "#display(standarddf1.groupBy(\"profession\").count())#DSL\n",
    "#Standardization2 - column uniformity\n",
    "standarddf2=standarddf1.withColumn(\"profession\",initcap(\"profession\"))#inicap or any other string function with columnOr name can accept either column or string type provided if the string is a column name for eg. profession/age/sourcesystem.\n",
    "display(standarddf2.limit(20))\n",
    "#display(standarddf2.groupBy(\"profession\").count())#DSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b5147c4-0b50-40c7-b5cc-882207c8745b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW standarddf2 AS\n",
    "SELECT\n",
    "  id,\n",
    "  firstname,\n",
    "  lastname,\n",
    "  age,\n",
    "  INITCAP(profession) AS profession,\n",
    "  sourcesystem\n",
    "FROM standarddf1;\n",
    "\n",
    "select * from standarddf2 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7a6af35-e4a0-42d5-aeda-4a77da91dcb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization3 - Format Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df522e40-e0e5-455d-9401-6440c079109a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Did analysis to understand the format issues in our id and age columns\n",
    "#standarddf2.where(\"id like 't%'\").show()\n",
    "standarddf2.where(\"id rlike '[a-zA-Z]'\").show()#rlike is regular expression like function that help us identify any string data in our DF column\n",
    "standarddf2.where(\"age rlike '[^0-9]'\").show()#checking for any non number values in age column\n",
    "#standarddf3=standarddf2.withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc0bc1bc-e8d9-4fa2-a451-977dfed781dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace,replace\n",
    "#Let's apply scrubbing features to our id column to replace ten with 10 (or we can think of using GenAI here)\n",
    "replaceval={'one':'1','two':'2','three':'3','four':'4','five':'5','six':'6','seven':'7','eight':'8','nine':'9','ten':'10'}\n",
    "standarddf3=standarddf2.na.replace(replaceval,subset=[\"id\"])\n",
    "display(standarddf3.limit(20))\n",
    "standarddf3=standarddf2.withColumn(\"age\",regexp_replace(\"age\",\"[^0-9]\",\"\"))\n",
    "display(standarddf3.limit(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e72fd178-ab89-4430-8bf5-a0d981c66de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW standarddf3 AS\n",
    "SELECT\n",
    "  CASE\n",
    "    WHEN cast(id as string) = 'nine'  THEN '9'\n",
    "    WHEN cast(id as string) = 'ten'   THEN '10'\n",
    "    ELSE id\n",
    "  END AS id,\n",
    "  firstname,\n",
    "  lastname,\n",
    "  REGEXP_REPLACE(age, '-', '') AS age,\n",
    "  profession,\n",
    "  sourcesystem\n",
    "FROM standarddf2;\n",
    "\n",
    "select * from standarddf3 limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7102f92-b61d-4c7e-9ec2-5fa41932c202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from standarddf3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "892d39b3-5276-4b63-ab7c-a86723c9d988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization4 - Data Type Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da284144-9c6e-4572-80d8-d1e3f3928dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf4=standarddf3.withColumn(\"id\",col(\"id\").cast(\"long\"))\n",
    "standarddf4=standarddf3.withColumn(\"age\",col(\"age\").cast(\"short\"))\n",
    "display(standarddf4.limit(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0079a522-bb7a-4425-a2e0-2fecbe1d9986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view standarddf4 as \n",
    "SELECT \n",
    "    CAST(id AS BIGINT) AS id,\n",
    "    firstname,\n",
    "  lastname,\n",
    "  CAST(age AS SMALLINT) AS age,\n",
    "  profession,\n",
    "  sourcesystem\n",
    "FROM standarddf3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938f772a-675e-45f9-9f45-541be2991854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from standarddf4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a17f44-4b74-4f50-992c-36f86fd52efc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization5 - Naming Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64abe4bb-d4b8-4f2c-8c13-05840b197c0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf5=standarddf4.withColumnRenamed(\"id\",\"custid\")\n",
    "standarddf5=standarddf4.withColumnsRenamed({\"id\":\"custid\",\"sourcesystem\":\"srcsystem\"})\n",
    "display(standarddf5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a457aa3d-37e8-4735-b0d0-8e054fc7904c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view standarddf5 as \n",
    "select id as custid,\n",
    "firstname,\n",
    "lastname,\n",
    "age,\n",
    "profession,\n",
    "sourcesystem as srcsystem\n",
    "from standarddf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a45fe3-76ac-4db6-a1e3-40d30e0442b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from standarddf5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b7c5040-cf71-45cf-ad85-fb460740ae7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization6 - Reorder Standadization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bce6c1f-efae-4b5f-ba55-d88126d7a6e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf6=standarddf5.select(\"custid\", \"age\", \"firstname\",\"lastname\",\"profession\",\"srcsystem\")\n",
    "#display(standarddf6)\n",
    "mungeddf=standarddf6\n",
    "display(mungeddf.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dae8c74-68d9-4306-96ca-102851cf3fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Try writing all these standardizations in one SQL..\n",
    "create or replace temp view mungeddf as \n",
    "SELECT \n",
    "    custid, \n",
    "    age, \n",
    "    firstname,\n",
    "    lastname,\n",
    "    profession,\n",
    "    srcsystem\n",
    "FROM standarddf5;\n",
    "\n",
    "SELECT * FROM mungeddf LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa5d745f-9c1e-4055-8a07-25e3b1e7b3f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**2. Data Enrichment** - Detailing of data\n",
    "Makes your data rich and detailed <br>\n",
    "a. Add (withColumn,select,selectExpr), Derive (withColumn,select,selectExpr), Remove(drop,select,selectExpr), Rename (withColumnRenamed,select,selectExpr), Modify/replace (withColumn,select,selectExpr) - very important spark sql functions <br>\n",
    "b. split, merge/Concat <br>\n",
    "c. Type Casting, reformat & Schema Migration <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "443509ba-d31b-432c-94cf-98e211e33bc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####a. Add (), Derive (), Rename (), Modify/replace (), Remove/Eliminate () - very important spark sql DF functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de66172d-f68b-4f35-90d8-b65cc252afba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Adding of columns\n",
    "Lets add datadt (date of the data orgniated from the source for eg. provided in the filename in a format of yy/dd/MM) and loaddt (date when we are loading the data into our system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af07b1b4-f75f-4709-b96f-fec8d6f73f9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "derived_datadt='25/30/12'\n",
    "print(f\"hello '{derived_datadt}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23c2d60c-7afc-47ef-aa1d-8a96d22a92f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,current_date#already imported, not needed here\n",
    "original_filename='custsmodified_25/30/12.csv'#We are deriving this date from the filename provided by the source custsmodified_25/30/12.csv\n",
    "derived_datadt=original_filename.split('_')[1].split('.')[0]\n",
    "#derived_datadt='25/30/12'#We are deriving this date from the filename provided by the source custsmodified_25/30/12.csv\n",
    "enrichdf1=mungeddf.withColumn(\"datadt\",lit('25/30/12')).withColumn(\"loaddt\",current_date())\n",
    "enrichdf1.printSchema()\n",
    "#or\n",
    "enrichdf1=mungeddf.withColumns({\"datadt\":lit('25/30/12'),\"loaddt\":current_date()})\n",
    "enrichdf1.printSchema()\n",
    "#or\n",
    "enrichdf1=mungeddf.select(\"*\",lit(derived_datadt).alias('datadt'),current_date().alias('loaddt'))#DSLs (FBP function)\n",
    "#or\n",
    "enrichdf1=mungeddf.selectExpr(\"*\",\"'25/30/12' as datadt\",\"current_date() as loaddt\")#DSL(select) + SQL expression\n",
    "enrichdf1=mungeddf.selectExpr(\"*\",f\"'{derived_datadt}' as datadt\",\"current_date() as loaddt\")#DSL(select) + SQL expression\n",
    "enrichdf1.printSchema()\n",
    "display(enrichdf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b037e107-3a6a-408e-9765-d2bfb333a305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#Without knowing too much of DSL functions, I can use my familiar language knowledge of SQL and in a single SQL i can achieve this entire enrichment.\n",
    "original_filename='custsmodified_25/30/12.csv'\n",
    "derived_datadt=original_filename.split('_')[1].split('.')[0]\n",
    "spark.sql(f\"\"\"\n",
    "    create or replace temp view enrichdf1 as \n",
    "    SELECT \n",
    "        *,\n",
    "        '{derived_datadt}' AS datadt,\n",
    "        current_date() AS loaddt\n",
    "    FROM mungeddf\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e0c10f-d4af-4da4-a664-57a3be35f28a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from enrichdf1 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60833518-5d6d-4d0a-92ed-56f73de1df0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Deriving of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af38ac71-186d-4da7-b040-203f7133ad4d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769015755063}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "enrichdf2=enrichdf1.withColumn(\"professionflag\",substring(\"profession\",1,1))\n",
    "#or\n",
    "enrichdf2=enrichdf1.select(\"*\",substring(\"profession\",1,1).alias(\"professionflag\"))\n",
    "#or\n",
    "enrichdf2=enrichdf1.selectExpr(\"*\",\"substr(profession,1,1) as professionflag\")\n",
    "display(enrichdf2.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf6c467-c100-4cb7-9517-3e7d73b49556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Rather than using DSL, a single Select can achieve add/remove/rename/replace/modify the given columns, so sql is better to use for performing all in one select sql\n",
    "create or replace temp view enrichdf2 as \n",
    "select *,substring(profession,1,1) as professionflag from enrichdf1;\n",
    "select * from enrichdf2 limit 10;\n",
    "    \n",
    "--Rather than using DSL, a single Select can achieve add/remove/rename/replace/modify the given columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8176ec5b-bed1-44cf-8fe4-591dd02f1c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Renaming of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2cdd5b3-e319-4f9d-87e0-eaf81186d5eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Can we use withColumn to rename? not directly, its costly\n",
    "enrichdf3=enrichdf2.withColumn(\"sourcename\",col(\"srcsystem\"))\n",
    "enrichdf3=enrichdf3.drop(\"srcsystem\").select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"sourcename\",\"datadt\",\"loaddt\",\"professionflag\")\n",
    "#or\n",
    "enrichdf3=enrichdf2.select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",col(\"srcsystem\").alias(\"sourcename\"),\"datadt\",\"loaddt\",\"professionflag\")#costly too, since we have to choose all columns in the select\n",
    "#or\n",
    "#enrichdf2.printSchema()\n",
    "enrichdf3=enrichdf2.selectExpr(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"srcsystem as sourcename\",\"datadt\",\"loaddt\",\"professionflag\")#costly too, since we have to choose all columns in the select\n",
    "#or\n",
    "enrichdf3=enrichdf2.withColumnRenamed(\"srcsystem\",\"sourcename\")#Best function to rename the column(s)\n",
    "#or\n",
    "enrichdf3=enrichdf2.withColumnsRenamed({\"srcsystem\":\"sourcename\",\"professionflag\":\"profflag\"})\n",
    "display(enrichdf3.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c12fde8-4245-4681-9998-3a014d73694b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view enrichdf3 as \n",
    "SELECT \n",
    "    custid,\n",
    "    age,\n",
    "    firstname,\n",
    "    lastname,\n",
    "    profession,\n",
    "    srcsystem AS sourcename, -- Renaming occurs here\n",
    "    datadt,\n",
    "    loaddt,\n",
    "    professionflag AS profflag -- Renaming occurs here\n",
    "FROM enrichdf2;\n",
    "\n",
    "SELECT * FROM enrichdf3 LIMIT 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b74d7a0-af39-4ca5-984b-0ac7dade292f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Modify/replace (withColumn, select/selectExpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93954ce3-f919-4b3e-a6f0-a36d55c01263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enrichdf4=enrichdf3.withColumn(\"profession\",col(\"sourcename\"))#This will replace the profession with sourcename\n",
    "#or\n",
    "enrichdf4=enrichdf3.withColumn(\"profession\",concat(\"profession\",lit('-'),\"profflag\"))#This will modify/enrich the profession column with sourcename\n",
    "#or using select/selectExpr\n",
    "enrichdf4=enrichdf3.select(\"custid\",\"age\",\"firstname\",\"lastname\",concat(\"profession\",lit('-'),\"profflag\").alias(\"profession\"),\"sourcename\",\"datadt\",\"loaddt\",\"profflag\")\n",
    "#or use selectExpr\n",
    "enrichdf4=enrichdf3.selectExpr(\"custid\",\"age\",\"firstname\",\"lastname\",\"concat(profession,'-',profflag) as profession\",\"sourcename\",\"datadt\",\"loaddt\",\"profflag\")\n",
    "display(enrichdf4.take(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d167328e-2ea3-41e4-9841-510c776ab472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view enrichdf4 as \n",
    "SELECT \n",
    "    custid,\n",
    "    age,\n",
    "    firstname,\n",
    "    lastname,\n",
    "    concat(profession, '-', profflag) AS profession, -- Overwrites/Modifies the profession column\n",
    "    sourcename,\n",
    "    datadt,\n",
    "    loaddt,\n",
    "    profflag\n",
    "FROM enrichdf3;\n",
    "\n",
    "SELECT * FROM enrichdf4 LIMIT 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "481e8906-acf6-4005-ab71-c0128219fb82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Remove/Eliminate (drop,select,selectExpr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5686e04-542a-4e5e-98c2-5369bc8a67f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#enrichdf4=enrichdf3.withColumn(\"profession\",col(\"sourcename\"))#Cannot be used\n",
    "#or using select/selectExpr (yes, but costly)\n",
    "enrichdf5=enrichdf4.select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"sourcename\",\"datadt\",\"loaddt\")\n",
    "#or use selectExpr (yes, but costly)\n",
    "enrichdf5=enrichdf4.selectExpr(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"sourcename\",\"datadt\",\"loaddt\")\n",
    "#or \n",
    "enrichdf5=enrichdf4.drop(\"profflag\")#right function to use from dropping\n",
    "display(enrichdf5.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d78298e0-9454-4ec4-93c8-f48a86020994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view enrichdf5 as \n",
    "SELECT \n",
    "    custid,\n",
    "    age,\n",
    "    firstname,\n",
    "    lastname,\n",
    "    profession,\n",
    "    sourcename,\n",
    "    datadt,\n",
    "    loaddt\n",
    "FROM enrichdf4;\n",
    "\n",
    "SELECT * FROM enrichdf5 LIMIT 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93b5ae8b-7b6d-4b55-8209-e600353c8f9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####b. Splitting & Merging/Melting of Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab2d5516-afcc-4ed1-94cb-a9cd3751d3a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Splitting of column\n",
    "splitdf=enrichdf5.withColumn(\"profflag\",split(\"profession\",'-'))\n",
    "splitdf=splitdf.withColumn(\"profession\",col(\"profflag\")[0])\n",
    "#splitdf=splitdf.withColumn(\"profflag\",col(\"profflag\")[1])\n",
    "#or\n",
    "splitdf=splitdf.withColumn(\"shortprof\",upper(substring(col(\"profession\"),1,3))).drop(\"profflag\")\n",
    "#Merging of column\n",
    "mergeddf=splitdf.select(col(\"custid\"),\"age\",concat(col(\"firstname\"),lit(\" \"),col(\"lastname\")).alias(\"fullname\"),\"profession\",\"sourcename\",\"datadt\",\"loaddt\",\"shortprof\")#usage of select will help us avoid chaining of withColumn,drop,select\n",
    "display(mergeddf.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef33038-24e6-41e6-a9cc-0c273e59e288",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view mergeddf as \n",
    "SELECT \n",
    "    custid,\n",
    "    age,\n",
    "    concat_ws(' ', firstname, lastname) AS fullname, -- Merging firstname and lastname\n",
    "    split(profession, '-')[0] AS profession, -- Splitting and taking the first element\n",
    "    sourcename,\n",
    "    datadt,\n",
    "    loaddt,\n",
    "    upper(substring(split(profession, '-')[0], 1, 3)) AS shortprof -- creating shortprof based on the split result\n",
    "FROM enrichdf5;\n",
    "\n",
    "SELECT * FROM mergeddf LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cffac420-960e-4f3a-a400-9ed7f05ac13e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####c. Formatting & Typecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc73e0f-2444-4da2-af58-72b130e4c824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "formatteddf=mergeddf.withColumn(\"datadt\",to_date(col(\"datadt\"),'yy/dd/MM'))#25/30/12 -> 2025-12-30\n",
    "formatteddf.printSchema()\n",
    "display(formatteddf.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d7138ab-6254-4bc5-8d60-39bd575a2468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--All the above enrichments can be writtern in a single SQL select.\n",
    "create or replace temp view formatteddf as \n",
    "SELECT \n",
    "    custid,\n",
    "    age,\n",
    "    fullname,\n",
    "    profession,\n",
    "    sourcename,\n",
    "    to_date(datadt, 'yy/dd/MM') AS datadt, -- Converts string '25/30/12' to Date '2025-12-30'\n",
    "    loaddt,\n",
    "    shortprof\n",
    "FROM mergeddf;\n",
    "SELECT * FROM formatteddf LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c064c2f-d637-4e34-bc50-e3bd11eafaa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Data Customization - Application of Tailored Business specific Rules <br>\n",
    "a. User Defined Functions <br>\n",
    "b. Building of Frameworks & Reusable Functions (We will learn very next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa2a03d0-8191-462d-bce0-c4411383b0a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#formatteddf2=formatteddf.withColumn(\"sourcename\",upper(\"sourcename\"))\n",
    "#formatteddf2.show(2)\n",
    "#Caveat - If there is no upper() function is available already in spark dsl/sql, we can either search for some functions in the online opensource platform or we have to create one (custom functions)\n",
    "#from org.apache.sql.functions import upperodd\n",
    "\n",
    "def upperodd(colname_containsvalue):\n",
    "    convertedcolvalue=colname_containsvalue.upper()\n",
    "    return convertedcolvalue\n",
    "print(upperodd(\"irfan\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e70246b-6fd3-4869-95c4-a7d839138eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#If I want to apply a python function on a SQL temp view?\n",
    "from pyspark.sql.functions import udf\n",
    "#udfupper=udf(upperodd) this is applicable only for DSL\n",
    "#Interview question asked: Have you used udf in spark sql? yes, by registering the python function to an UDF function\n",
    "spark.udf.register(\"udfuppersql\",upperodd) #for SQL udf we have to register the python function to an UDF function\n",
    "rawdf1.createOrReplaceTempView(\"rawdf11\")\n",
    "spark.sql(\"select * from rawdf11\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e450e2a-cd52-4eeb-8d86-45bdcff1fc6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "formatteddf2=rawdf1.withColumn(\"firstname\",upper(col(\"firstname\")))#we can't run python function as it is\n",
    "formatteddf2.explain()\n",
    "#display(formatteddf2.take(10))#prefer\n",
    "from pyspark.sql.functions import udf\n",
    "udfupper=udf(upperodd)#promote normal python function to spark ready udf\n",
    "formatteddf2=rawdf1.withColumn(\"firstname\",udfupper(col(\"firstname\")))#if udf is inevitable, then we create despite of performance bottleneck\n",
    "formatteddf2.explain()\n",
    "display(formatteddf2.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c39873c4-8305-4fb6-bc66-22be3a0b1022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Create Python Custom Function with complex logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dafc259a-0951-4beb-84a5-6c36efe326e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Calculating age category from the given age of the customer\n",
    "def pythonAgeCat(dfcol):\n",
    "    if dfcol is None:\n",
    "        return \"Unknown\"\n",
    "    elif dfcol<=10:\n",
    "        return \"child\"\n",
    "    elif dfcol>10 and dfcol<=18:\n",
    "        return \"teenager\"\n",
    "    elif dfcol>18 and dfcol<=30:\n",
    "        return \"young\"\n",
    "    elif dfcol>30 and dfcol<=50:\n",
    "        return \"middleaged\"\n",
    "    else:\n",
    "        return \"senior\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f1e413b-555f-4ea7-8631-702a98b0b8a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "sparkudfageCat=udf(pythonAgeCat)\n",
    "customdf=formatteddf.withColumn(\"agecat\",sparkudfageCat(\"age\"))\n",
    "display(customdf.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfce0558-e298-40f1-8816-02827a6231ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.udf.register(\"sql_age_cat\", pythonAgeCat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc9da922-faab-4f59-8c73-762da1ae29e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view customdf as \n",
    "SELECT \n",
    "    *,\n",
    "    sql_age_cat(age) AS agecat\n",
    "FROM formatteddf;\n",
    "\n",
    "SELECT * FROM customdf LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6906ced7-ca3d-4470-9599-4715d13cd809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Data Curation/Processing (Pre Wrangling Stage) - Applying different levels of business logics, transformation, filtering, grouping, aggregation and limits applying different transformation functions\n",
    "1. Select, Filter\n",
    "2. Derive flags & Columns\n",
    "3. Format\n",
    "4. Group & Aggregate\n",
    "5. Limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48cd6762-361e-44e0-afb6-e7a5eebc8e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Select, Filter\n",
    "In terms of Performance Optimzation - I ensured to do Push Down Optimization by doing select(project) & Filter(predicate) of what ever the expected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8ea1dc6-5ae0-4a42-a091-6e076fdbaa0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "selectdf=customdf.select(\"custid\",\"age\",col(\"profession\").alias(\"prof\"),\"agecat\")\n",
    "display(selectdf)\n",
    "selectdf=customdf.selectExpr(\"custid\",\"age\",\"profession as prof\",\"agecat\")\n",
    "display(selectdf.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a871429c-7cd1-4697-ba3d-970b6dbd030c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Select\n",
    "#select, functions, case, literal ,from,where,group by, having, order by, limit...\n",
    "#Select few columns by filtering few rows\n",
    "selectdf=customdf.select(\"custid\",\"age\",\"agecat\",col(\"profession\").alias(\"prof\"),\"agecat\")#DSL Select\n",
    "selectdf.show(5)\n",
    "selectdf=customdf.selectExpr(\"custid\",\"age\",\"agecat\",\"profession as prof\",\"agecat\")#SQL Select\n",
    "selectdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be7afc34-ccd3-4b6a-b698-ed43bb7b37b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Filter/Where - both are literally same (filter will be used by FBP developers & where will be used by SQL developers)\n",
    "filterdf=selectdf.filter((col(\"age\")>40) & (col(\"age\")<=60))#DSL operation\n",
    "filterdf.show(5)\n",
    "filterdf=selectdf.where((col(\"age\")>40) & (col(\"age\")<=60))#DSL operation\n",
    "filterdf.show(5)\n",
    "\n",
    "filterdf=selectdf.filter(\"age>40 and age<=60\")#SQL where operation\n",
    "filterdf.show(5)\n",
    "filterdf=selectdf.where(\"age>40 and age<=60\")#SQL where operation\n",
    "filterdf.show(5)\n",
    "#filterdf.write.saveAsTable(\"filtercust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf895deb-10c5-4c47-9380-4c4c4056e1a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Prefer SQL than DSL because of familiarity\n",
    "create or replace temp view filterdf as \n",
    "select * from customdf\n",
    "where age>40 and age<=60;\n",
    "select * from filterdf;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c4c4c5-35e0-4bd7-babe-e7c23330621c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Derive flags & Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b23ba97e-b18a-481f-b56c-7d008cbced82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#We have created agecat using UDF (which is supposed to use only if it is inevitable)\n",
    "#But we can do the same using DSL When.otherwise or SQL CASE WHEN\n",
    "#Deriving Flag\n",
    "#Syntax in DSL: when(conditions,\"value\").when(conditions,\"value2\").otherwise(\"valuen\").alis(\"colname\")\n",
    "curateddf=customdf.select(\"*\",when(col(\"age\").isNull(),\"U\").\n",
    "                          when(col(\"age\")<=10,\"C\").\n",
    "                          when((col(\"age\")>10) & (col(\"age\")<=18),\"T\").\n",
    "                          when((col(\"age\")>18) & (col(\"age\")<=30),\"Y\").\n",
    "                          when((col(\"age\")>30) & (col(\"age\")<=50),\"M\").\n",
    "                          otherwise(\"S\").alias(\"agecatflag\"))#Suggessted than using UDFs\n",
    "display(curateddf.take(10))\n",
    "#Deriving Column\n",
    "#Syntax in SQL: case when conditions then value when conditions then value2 else valuen end as colname\n",
    "curateddf=curateddf.drop(\"agecat\").selectExpr(\n",
    "    \"*\",\"\"\"case when age is null then 'Unknown' \n",
    "                              when age<=10 then 'child' \n",
    "                              when age>10 and age<=19 then 'teenager' \n",
    "                              when age>19 and age<=30 then 'young'\n",
    "                              when age>30 and age<=50 then 'middleaged'\n",
    "                              else 'oldaged' end as agecat\"\"\")#Suggessted than using UDFs\n",
    "display(curateddf.take(10))\n",
    "#Interview Answer of how you optimized the existing spark code developed by your ex team members?\n",
    "#I analysed the existing udfs used in my project and seeked for opurtunities to convert them into SQL/dsl based programs by implementing the udf logics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f5e2492-4a3f-45f5-91c3-d85d48de61db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--DSL syntax is not familiar and complex, sql is familiar and simple(no need to use brackets causiously as like DSL)\n",
    "create or replace temp view curateddf as \n",
    "select *,case when age is null then 'Unknown' \n",
    "                              when age<=10 then 'child' \n",
    "                              when age>10 and age<=19 then 'teenager' \n",
    "                              when age>19 and age<=30 then 'young'\n",
    "                              when age>30 and age<=50 then 'middleaged'\n",
    "                              else 'oldaged' end as agecat1 from customdf ;\n",
    "select * from curateddf;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2ae901-1866-46bc-befb-b1c63075bb3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####3.Format (Deriving Columns with different format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2facf0b7-5218-4a13-bce8-80c73a82805c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769094987292}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We can use different functions - string or number or date function for format modeling\n",
    "curateddf3=curateddf.select(\"*\",datediff(\"loaddt\",\"datadt\").alias(\"delaydays\"),year(\"datadt\").alias(\"datayear\")\n",
    "                            ,month(\"datadt\").alias(\"datamonth\")\n",
    "                            ,last_day(\"datadt\").alias(\"datalastday\")).withColumn(\"agecat\",initcap(\"agecat\"))\n",
    "display(curateddf3.take(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4392fd05-2c61-4817-a4b4-74ae2c39cf6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view curateddf3 as \n",
    "select *,\n",
    "datediff(loaddt,datadt) as delaydays,year(datadt) as datayear\n",
    "                            ,month(datadt) as datamonth\n",
    "                            ,last_day(datadt)as datalastday,initcap(agecat) as agecat2 from curateddf;\n",
    "select * from curateddf3;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0125396b-d2a8-4d73-aeb9-5cc406ba6da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####4. Group & Aggregate\n",
    "Before performing grouping or aggr, consider the below factors from the dataset....<br>\n",
    "identifier?\tcid (high in cardinality/difference) (surrogate/naturalkey)<br>\n",
    "descriptive?\tname<br>\n",
    "metric?\tavg(age),count(distinct cid),max(age),min(age)<br>\n",
    "measure?\tage,cid<br>\n",
    "grouping?\tage,prof - low in cardinality/difference<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17e024dd-0be3-45a6-88ff-07ab39c93e0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#from pyspark.sql.functions.aggregate import avg,count,initcap,last_day,datediff,year,month,agg\n",
    "#What is the total number of customers we have?\n",
    "print(curateddf3.count())\n",
    "#What is the total number of customers we have in each profession?\n",
    "curateddf4=curateddf3.groupBy(\"profession\").count()\n",
    "display(curateddf4.take(100))\n",
    "#Multiple Aggregation with one grouping - What is the total number of customers,average age of those customers we have in each profession?\n",
    "curateddf4=curateddf3.groupBy(\"profession\").avg(\"age\").withColumnRenamed(\"avg(age)\",\"avgage\")\n",
    "display(curateddf4.take(100))\n",
    "#To calculate multiple aggregation, we need to use a function called agg function\n",
    "curateddf4=curateddf3.groupBy(\"profession\").agg(count(\"custid\").alias(\"custcount\"),avg(\"age\").alias(\"avgage\"))\n",
    "display(curateddf4.take(100))\n",
    "#curateddf4 this dataframe we materialize/store in some tables/files later\n",
    "#Multiple Aggregation with multiple grouping - What is the total number of customers,average age of those customers we have in each profession?\n",
    "curateddf4=curateddf3.groupBy(\"profession\",\"agecat\").\\\n",
    "agg(count(\"custid\").alias(\"custcount\"),avg(\"age\").alias(\"avgage\"))\n",
    "display(curateddf4.take(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41612151-5838-4ac0-a55d-0cf10202a8ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--It depends to write in SQL or DSL, which ever is familiar\n",
    "--curateddf4.groupBy('profession','agecat').agg(count('custid').alias(\"custcount\"),avg(\"age\").aliash(\"avgage\"))\n",
    "create or replace temp view curateddf4 as \n",
    "select profession,agecat,count(custid) as custcount,avg(age) as avgage\n",
    "from curateddf3\n",
    "group by profession,agecat;\n",
    "select * from curateddf4 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9131b72e-0161-4355-9432-c3594aa852cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####5. Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3c1e9a6-ec1b-4cfc-8a76-6ba565cbc466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "curateddf5=curateddf4.orderBy(\"profession\",\"agecat\")\n",
    "display(curateddf5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7f65868-4f47-49c7-964d-2e69ac84e7b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view curateddf5 as \n",
    "select *\n",
    "from curateddf4\n",
    "order by profession,agecat;\n",
    "select * from curateddf5 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "199eef62-da2e-4190-8a8e-ecdbb836773b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####6. Limit\n",
    "Let us take an oppurtunity to understand different data limiting/restricting functions<br>\n",
    "Limit is a dataframe TRANSFORMATION functions used to limit the number of rows returned in a spark dataframe FORMAT<br>\n",
    "Take is a dataframe/RDD ACTION functions used to limit the number of rows returned in a python list FORMAT<br>\n",
    "display is a standard output databricks specific function used to produce entire DF/List output in a notebook view with multiple options <br>\n",
    "show is a standard output spark dataframe specific function used to produce default 20 rows of a DF output in a notebook/REPL/IDE view <br>\n",
    "collect is a spark action that help us collect the DF/RDD data into the driver environment in a form of python list <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f300999f-4475-42e4-8f04-7b683adf241f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#anything can be used under display()\n",
    "print(\"limit output\")\n",
    "curateddf5.limit(20).show(10)\n",
    "print(\"take output\")\n",
    "curateddf5.take(10)\n",
    "#When to use what\n",
    "#I have to filter some data in a limited dataset of 100 rows\n",
    "curateddf5.limit(100).filter(\"profession='Accountant'\").show()\n",
    "#Display\n",
    "display(curateddf5.limit(100).filter(\"profession='Accountant'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97e9f826-4902-4c82-9a47-e36d9203feb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Data Wrangling - More of Analytics + Transformation\n",
    "  1. Joins - Relation/Connection established between one or more datasets/df/tabl to produce the broader/extended view of the data horizontally.\n",
    "  5 Categories of Joins:\n",
    "  inner, outer(left\n",
    "  right, full), self, cross, special optimized (semi, anti)\n",
    "  2. Lookup\n",
    "  3. Lookup & Enrichment\n",
    "  4. Schema Modeling  (Denormalization)\n",
    "  5. Windowing\n",
    "  6. Analytical\n",
    "  7. Set operations\n",
    "  8. grouping & aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93316b2d-99b6-4567-a73d-edb6c2e63d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1. Joins\n",
    "Joins are Relation/connection of one or more tables to perform widened (horizontal) data analytics\n",
    "1. Frequently used simple joins (inner, left)\n",
    "2. InFrequent simple joins (self, right, full, cartesian)\n",
    "3. Advanced joins (Semi and Anti)<br>\n",
    "Syntax - dfleft.join(dfright,how='typeofjoin',on='custid'=='cid')\n",
    "4. Optimized joins (Broadcast join, SMB Join, Shuffle hash join, Map/Reduceside join, Skewed join etc.)-AQE (Adaptive Query Execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f5c77d-7dd5-4260-b58b-bc293c319c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,col\n",
    "#How to write join syntax in Spark and learn the semantics of join in spark\n",
    "rawdf1=spark.read.csv(\"/Volumes/we47catalog1/we47db1/we47volume1/custsmodified\",header=False,inferSchema=True).toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\")\n",
    "rawdf1=rawdf1.na.drop().where(\"id<>'ten' and id<>'trailer_data:end of file'\")\n",
    "rawdf1=rawdf1.where(\"id<>'ten' and id<>'trailer_data:end of file'\").na.drop()\n",
    "leftdf=rawdf1.select(\"id\",\"age\",\"firstname\",\"lastname\").where(\"id in (4000100,4000101)\")\n",
    "rightdf=rawdf1.select(\"id\",\"profession\").where(\"id in (4000100,4000102,4000103)\")\n",
    "leftdf.show(20,False)\n",
    "rightdf.show(20,False)\n",
    "#Let's understand all types of joins syntax & semantics quickly\n",
    "#dfleft.join(dfright,how='typeofjoin',on='custid'=='cid')\n",
    "\n",
    "#5 Categories of Joins : 1. inner(important), 2. outer(left(important), right, full), 3. special optimized(important) (semi, anti), 4. self, 5. cross\n",
    "#1. inner, 2. outer(left, right, full)\n",
    "#1. inner\n",
    "#Usecase - Ensure which ever the customers (from both df) matches - Functionality(application)\n",
    "print(\"inner\")\n",
    "innerjoindf=leftdf.join(rightdf,how='inner',on='id')\n",
    "innerjoindf.show(20)#only 4000100 from both df is displayed\n",
    "\n",
    "#2. outer(left, right, full)\n",
    "#2. left\n",
    "#Usecase - Ensure all of the master customers with/without profession provided - Functionality(application)\n",
    "print(\"left\")\n",
    "leftjoindf=leftdf.join(rightdf,how='left',on='id')#syntax(how to do)\n",
    "leftjoindf.show(20)#semantics(what is the output) - only 4000100,4000101 from both df is displayed with non applicable nulls in the right\n",
    "\n",
    "#2. right\n",
    "#Usecase - Ensure all of the customers with professions has to be displayed\n",
    "print(\"left\")\n",
    "rightjoindf=leftdf.join(rightdf,how='right',on='id')#syntax(how to do)\n",
    "rightjoindf.show(20)#semantics(what is the output) - only 4000100,4000102,4000103 from both df is displayed with non applicable nulls in the left\n",
    "\n",
    "#2. full\n",
    "#Usecase - Ensure all of the customers with/without master customers or professions has to be displayed\n",
    "print(\"full\")\n",
    "fulljoindf=leftdf.join(rightdf,how='full',on='id')#syntax(how to do)\n",
    "fulljoindf.show(20)#semantics(what is the output) - displays all 4000100,4000101,4000102,4000103 from both df is displayed with non applicable nulls from both left and right\n",
    "\n",
    "#3. special optimized (semi, anti)\n",
    "#Why semi/anti is an optimized join?\n",
    "#Interview answer - I found low hanging fruits oppurtunities in my project.. converting inner/left joins to semi/anti, because the joins are good in performance (it uses exists condition rather than in condition)\n",
    "#semi\n",
    "#Usecase - Ensure which ever the customers (from both df) matches - Functionality(application)\n",
    "print(\"left semi/semi\")\n",
    "semijoindf=leftdf.join(rightdf,how='leftsemi',on='id')\n",
    "semijoindf.show(20)#only 4000100 (matches between both df) from LEFT df is displayed\n",
    "\n",
    "#anti\n",
    "#Usecase - Ensure which ever the customers (from both df) matches - Functionality(application)\n",
    "print(\"left anti/anti\")\n",
    "antijoindf=leftdf.join(rightdf,how='leftanti',on='id')\n",
    "antijoindf.show(20)#only 4000101 (un matched between both df) from LEFT df is displayed\n",
    "\n",
    "#4. Self join\n",
    "#Usecase - Hierarchical Retrival or join - Data joined by itself to produce the relational output of the self dataset\n",
    "print(\"self\")\n",
    "custaffliatedf=leftdf.withColumn(\"refcustid\",lit('4000100'))\n",
    "custaffliatedf.show(3)\n",
    "selfjoindf=custaffliatedf.alias(\"l\").join(custaffliatedf.alias(\"r\"),how='inner',on=(col('l.id')==col(\"r.refcustid\")))\n",
    "\n",
    "selfjoindf.select('l.id','r.*').show(20)#only 4000101 (un matched between both df) from LEFT df is displayed\n",
    "\n",
    "#5. cross join\n",
    "#What join it will perform by default? inner join (if we use on condition), cartesian/cross join (if no on condition used)\n",
    "#Minimum syntax to use?\n",
    "#cross\n",
    "print('cartesian/cross')\n",
    "joindf=leftdf.join(rightdf)#without on it is cross/cartesian product (very costly and avoidable join)\n",
    "joindf.show(20)#6 rows returned\n",
    "\n",
    "print('inner by default')\n",
    "joindf=leftdf.join(rightdf,on='id')#default inner join\n",
    "joindf.show(20)#4000100 returned\n",
    "\n",
    "print('expecting right join, but because of lack of on, cross join happened')\n",
    "joindf=leftdf.join(rightdf,how='right')#without on it is cross/cartesian product (very costly and avoidable join)\n",
    "joindf.show(20)#6 rows returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "897e84c2-cbbc-48cd-b36b-5a3ecd3b7a3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import *\n",
    "#We already have customer master (Qualifying/dimension) data in a curated state\n",
    "curateddf3.show(5)#This contains only customer curated data (munged, enriched, customized, curated)\n",
    "#We need to bring transaction detailed (Quantifying/Fact) data upto curated state (we need to follow all/any/none of the steps that we followed so far munged, enriched, customized, curated, wrangle etc., depends on the EDA result)\n",
    "#strt1=\"txnid long,txndt string,custid int,amt double,category string,product string,city string,state string,spendby string\"\n",
    "strt1=StructType([StructField('txnid', IntegerType(), True), StructField('txndt', StringType(), True), StructField('custid', IntegerType(), True), StructField('amt', DoubleType(), True), StructField('category', StringType(), True), StructField('product', StringType(), True), StructField('city', StringType(), True), StructField('state', StringType(), True), StructField('spendby', StringType(), True)])\n",
    "#txnsrawdf=spark.read.csv(\"/Volumes/workspace/default/volumewd36/txns_2025.txt\",inferSchema=True,header=False)\n",
    "#txnsrawdf.schema\n",
    "txnsrawdf=spark.read.schema(strt1).csv(\"/Volumes/we47catalog1/we47db1/we47volume1/txns\",header=False)\n",
    "txnsrawdf.printSchema()\n",
    "txnsrawdf.show(5)\n",
    "txnsrawdf.summary().show(100)\n",
    "#As per the above output, we can do some transformations of munging, enrichment etc.,\n",
    "txnsmungeddf=txnsrawdf.na.drop().dropDuplicates([\"txnid\"])\n",
    "txnsenrichdf=txnsmungeddf.withColumn(\"surrogatekey\",monotonically_increasing_id())\n",
    "txnscurateddf=txnsenrichdf.withColumn(\"txndt\",to_date(col(\"txndt\"),\"MM-dd-yyyy\"))\n",
    "txnscurateddf.printSchema()\n",
    "txnscurateddf.show(5)#Completed munging, enrichment and curation\n",
    "\n",
    "#Now lets achieve the solution for the business requirement:\n",
    "#Requirement: I need to do analytics/reporting of top 3 customers who did highest amount of transactions in our business last month, so i can send them some offers.\n",
    "#spark.sql(\"select month(add_months(current_date(), -1))\").show()\n",
    "txnsdecdf=txnscurateddf.where(\"month(txndt)=month(add_months(current_date(), -1))\")\n",
    "print(\"dec transactions count \",txnsdecdf.count())\n",
    "\n",
    "custdimdf=curateddf3\n",
    "joineddf=custdimdf.join(txnsdecdf,how=\"inner\",on=\"custid\")\n",
    "joineddf.show(10)\n",
    "#We have to generate a reporting table with only custid,name,profession,amount,spendby\n",
    "reportdf=joineddf.select(\"custid\",\"fullname\",\"amt\",\"spendby\").orderBy([\"amt\"],ascending=[False]).limit(3)\n",
    "reportdf.show()#Top 3 transacting customer info (in the overall data)\n",
    "reportdf=joineddf.select(\"custid\",\"fullname\",\"amt\",\"spendby\").orderBy([\"amt\"],ascending=[True]).limit(3)\n",
    "reportdf.show()#Least 3 transacting customer info (in the overall data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4423573-2d43-41ba-98cb-9960a641c1ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######1. Lookup (only exists/non exists check) - semi or anti\n",
    "Lookup is the process of looking up for some data attributes using the key to identify the presence of the values (not the actual values are returned)\n",
    "Eg. whether this particular customer made a transaction or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0786dfe-cecc-4088-85e1-b7164f51266b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Lookup: Show me only the customer information, who did transactions last month (i dont need what transaction)?\n",
    "#What type of best join i have to use? semi join is best and faster\n",
    "custonlylastmonthtransacteddf=custdimdf.join(txnsdecdf,how='semi',on='custid')\n",
    "custonlylastmonthtransacteddf.show(2)\n",
    "print(custonlylastmonthtransacteddf.count())\n",
    "#I store this data in a table, my business analysts will apply filter a particulatr customer\n",
    "\n",
    "#It is possible to produce the same result by using inner or left join also, but not preferred just for lookup\n",
    "custonlylastmonthtransacteddf=custdimdf.alias(\"cust\").join(txnsdecdf,how='inner',on='custid')\n",
    "resultdf=custonlylastmonthtransacteddf.select(custdimdf[\"*\"]).distinct()\n",
    "print(resultdf.count())\n",
    "\n",
    "custonlylastmonthtransacteddf=custdimdf.alias(\"cust\").join(txnsdecdf,how='left',on='custid')\n",
    "resultdf=custonlylastmonthtransacteddf.select(custdimdf[\"*\"]).where(\"txnid is not null\").distinct()\n",
    "print(resultdf.count())\n",
    "\n",
    "#Lookup: Show me only the customer information, who did not do transactions last month?\n",
    "custonlylastmonthnontransacteddf=custdimdf.join(txnsdecdf,how='anti',on='custid')\n",
    "custonlylastmonthnontransacteddf.show(2)\n",
    "print(custonlylastmonthnontransacteddf.count())\n",
    "\n",
    "custonlylastmonthtransacteddf=custdimdf.alias(\"cust\").join(txnsdecdf,how='left',on='custid')\n",
    "resultdf=custonlylastmonthtransacteddf.select(custdimdf[\"*\"]).where(\"txnid is null\").distinct()\n",
    "print(resultdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a58f3e2-aa03-4da4-8d40-901ee6e28788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custdimdf.createOrReplaceTempView(\"custdimdftv\")\n",
    "txnsdecdf.createOrReplaceTempView(\"txnsdecdftv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b022c023-f3c7-4334-a115-27ad8efeb569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--To overcome these below column name challenges, just use SQL rather than DSL\n",
    "--We have to give alias for dataframe if both join df contains some other common columns like profession is present in both dfs, other than the join column 'custid'\n",
    "--custonlylastmonthtransacteddf=custdimdftv.alias(\"cust\").join(txnsdecdftv.alias(\"t\"),how='semi',on='custid')\n",
    "--custonlylastmonthtransacteddf.select(cust.\"profession\",t.\"profession\")#If i don't use alias, then column ambiguiously defined error will occur\n",
    "create or replace temp view custonlylastmonthtransacteddf as \n",
    "SELECT cust.* \n",
    "--cust.age,cust.fullname,cust.profession,cust.sourcename,cust.datadt, cust.loaddt,cust.shortprof,cust.agecatflag,cust.agecat,t.*\n",
    "FROM custdimdftv cust\n",
    "LEFT SEMI JOIN txnsdecdftv t ON cust.custid = t.custid;\n",
    "select * from custonlylastmonthtransacteddf limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71b624bc-0759-4f3a-8e59-ee94a0fe8ae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Lookup & Enrichment: Show me both the customer information and transactions information of last month (i need what are the transactions made by the given customer)?\n",
    "#What type of best join i have to use? inner (only those who did transactions) or left join (all customers)\n",
    "#left\n",
    "lookendf=custdimdf.join(txnsdecdf,how='left',on=\"custid\")\n",
    "lookendf.show(10)\n",
    "print(lookendf.count())\n",
    "#inner\n",
    "enrichdf=custdimdf.join(txnsdecdf,how='inner',on=\"custid\")\n",
    "display(enrichdf)\n",
    "print(enrichdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbcbd36a-9706-46b9-af4d-f7e1048af562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "%sql\n",
    "create or replace temp view custonlylastmonthtransacteddf as \n",
    "SELECT cust.age,cust.fullname,cust.profession,cust.sourcename,cust.datadt, cust.loaddt,cust.shortprof,cust.agecatflag,cust.agecat,t.*\n",
    "FROM custdimdftv cust\n",
    "LEFT JOIN txnsdecdftv t ON cust.custid = t.custid;\n",
    "select * from custonlylastmonthtransacteddf limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a5aaf89-f320-438b-9010-86c044b6a783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######3.Schema Modeling (Denormalization-joined result of tables)-DWH/DataMart<br>\n",
    "Use inner(mostly),left/right/full(we can use depends on the business)\n",
    "We can build Datawarehouse components (dimension, fact tables) appling joins on the tables to achieve different types of schemas\n",
    "1. Star Schema\n",
    "txns_fact cust_mast_dim(dim)    \n",
    "id,amt    cid,name,age,city_lived \n",
    "1,100     11,irfan,44,Bangalore|Chennai\n",
    "2. Snowflake Schema - \n",
    "txns_fact cust_mast_dim(dim)    cust_det_dim(subdim)\n",
    "id,amt    cid,name,age          cid,city_lived\n",
    "1,100     11,irfan,44           11,Bangalore\n",
    "                                11,Chennai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f5678c0-2db4-4012-87ec-f1b260d66f40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Implementing Star schema model\n",
    "#In our current dataset, we have what schema model can be defined? Only star schema is possible, because we have only one fact table and multiple dimension tables (no sub dimension).\n",
    "#What is the best way to join all the tables? We can use inner join, because we have only one fact table and multiple dimension tables.\n",
    "#inner\n",
    "denormalized_fat_wide_df=custdimdf.join(txnsdecdf,how='inner',on='custid')\n",
    "denormalized_fat_wide_df.show(2)\n",
    "#I store this denormalized dataframe data into final fact table (in a single table) to simplify my customer queries without applying joins.\n",
    "print(denormalized_fat_wide_df.count())\n",
    "\n",
    "#or We can use left join also to ensure all customer info is captured.\n",
    "#left\n",
    "denormalized_fat_wide_df=custdimdf.join(txnsdecdf,how='left',on='custid')\n",
    "denormalized_fat_wide_df.show(2)\n",
    "print(denormalized_fat_wide_df.count())\n",
    "\n",
    "#or We can use full join also to ensure all customer (and+or) transaction info is captured.\n",
    "#left\n",
    "denormalized_fat_wide_df=custdimdf.join(txnsdecdf,how='full',on='custid')\n",
    "denormalized_fat_wide_df.show(2)\n",
    "print(denormalized_fat_wide_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b915d6a6-6e73-4df0-a8d8-dfb457e8be4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####4.Windowing Functionalities\n",
    "Is the concept of grouping/bucketize/dividing/partitioning and performing some analytical operation on the literally partitioned data\n",
    "Benifits of Windowing Functionality\n",
    "1. Creating Surrogate/primary key/seq number\n",
    "2. Performing Top N Analysis\n",
    "3. Duplicate Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3342ba1-87ab-4bb6-9646-c2530a3f1885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "denormalized_fat_wide_df=custdimdf.join(txnsdecdf,how='inner',on='custid')\n",
    "print(denormalized_fat_wide_df.count())\n",
    "orderjoineddf=denormalized_fat_wide_df.where(\"custid in (4000022,4000816)\").select(\"custid\",\"age\",\"profession\",\"txndt\",\"amt\",\"category\",\"product\",\"city\",\"state\",\"spendby\")\n",
    "#display(orderjoineddf)\n",
    "\n",
    "#We are using row_number() window function (very important)\n",
    "#Synax for windowing function\n",
    "#from pyspark.sql.window import Window\n",
    "#select(row_number().over(Window.partitionBy(\"custid\").orderBy(\"txndt\"))).alias(\"seqnum\"))\n",
    "\n",
    "#1.Creating Surrogate/primary key/seq number\n",
    "#Let us perform a non windowing operation that means we are not using partitioning\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.orderBy(\"custid\",\"txndt\")))#overall sorting\n",
    "display(sk_orderjoinedf)\n",
    "\n",
    "#2. Performing Top N Analysis\n",
    "#Let us perform windowing operation\n",
    "#Interview Questions pattern:\n",
    "print(\"a. Tell me the top 1 transaction made by the customer\")\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\").orderBy(desc(\"amt\")))).where(\"seqnum=1\")\n",
    "display(sk_orderjoinedf)\n",
    "print(\"b. Tell me the least 1 transaction made by the customer\")\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\").orderBy(\"amt\"))).where(\"seqnum=1\")\n",
    "display(sk_orderjoinedf)\n",
    "print(\"c. Tell me the top 2 transaction made by the customer\")\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\").orderBy(desc(\"amt\")))).where(\"seqnum<=2\")\n",
    "display(sk_orderjoinedf)\n",
    "print(\"d. Tell me the just the 2nd highest transaction made by the customer\")\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\").orderBy(desc(\"amt\")))).where(\"seqnum=2\")\n",
    "display(sk_orderjoinedf)\n",
    "print(\"e. Tell me the just the 2nd highest transaction made by the customer within the given state\")\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\",\"state\").orderBy(desc(\"amt\")))).where(\"seqnum=2\")\n",
    "display(sk_orderjoinedf)\n",
    "\n",
    "print(\"f. Show me the very first time the business made by the given customers?\")\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\").orderBy(\"txndt\"))).where(\"seqnum=1\")\n",
    "display(sk_orderjoinedf)\n",
    "print(\"g. Show me the very first two transaction made by the given customers?\")\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\").orderBy(\"txndt\")))\\\n",
    ".where(\"seqnum<=2\")\n",
    "display(sk_orderjoinedf)\n",
    "\n",
    "#3. Duplicate Handling\n",
    "#I want to remove duplicates based on custid with same spendby (for a given customer i need only one credit and one cash transaction info)\n",
    "print(\"I want to get the duplicate custid with the same spendby removed out from our data?\")\n",
    "#display(sk_orderjoinedf.coalesce(1).dropDuplicates([\"custid\",\"spendby\"]))\n",
    "#Using windowing function we can do controlled way of dropping duplicates, for eg. i want to drop the repeating data of same customer and spendby by retaining only the latest transactions\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\",\"spendby\").orderBy(desc(\"txndt\"))))\n",
    "display(sk_orderjoinedf)\n",
    "display(sk_orderjoinedf.where(\"seqnum=1\"))\n",
    "\n",
    "#but if amt is same for 2 transaction of diff dates? - rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "552115da-055b-4704-b7eb-276f36b6d791",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Second important window functions are rank() - used for applying same rank for same values and will have gaps (don't maintain continuity) \n",
    "# and dense_rank() - used for applying same rank for same values and will not have gaps (maintain denser/close continuity) \n",
    "print(\"a. Tell me the just the 2nd highest transaction made by the customer\")\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"rnk\",rank().over(Window.partitionBy(\"custid\").orderBy(desc(\"spendby\")))).withColumn(\"densernk\",dense_rank().over(Window.partitionBy(\"custid\").orderBy(desc(\"spendby\"))))\n",
    "display(sk_orderjoinedf)\n",
    "orderjoineddf.createOrReplaceTempView(\"orderjoineddftv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f40f260-4d5a-4cfc-a6f0-ff3bee255279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--SQL is simpler, doesn't require multiple libraries like window, desc etc.,\n",
    "--orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\").orderBy(desc(\"amt\"))))\n",
    "select *,row_number() over(partition by custid order by txndt desc) as rno,\n",
    "rank() over(partition by custid order by txndt desc) as rnk,\n",
    "dense_rank() over(partition by custid order by txndt desc) as drnk\n",
    "from orderjoineddftv \n",
    "limit 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcaed06c-132c-43f4-a482-3e6a0821fdd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(8) as dedup_count from(select *,row_number() over(partition by custid order by amt desc) as rownum from orderjoineddftv) as inlineview where rownum=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f4ff01-f3e3-4fec-870b-0c1b523d5af5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select *,row_number() over(partition by custid order by spendby desc)as rownum,rank() over(partition by custid order by spendby desc) as rak,dense_rank() over(partition by custid order by spendby desc) as dence from orderjoineddftv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6caff2b9-56da-4abb-9a3c-7772458b57b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####5.Analytical Functionalities\n",
    "Performing analytics/summarization/categorization of data by applying/not by applying windowing functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e82ac534-880d-4b17-b04b-ea229c37d577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Comparitive analytics - lead & lag\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"nexttransamt\",lead(\"amt\",1,0).over(Window.partitionBy(\"custid\").orderBy(asc(\"txndt\")))).withColumn(\"priortransamt\",lag(\"amt\",1,0).over(Window.partitionBy(\"custid\").orderBy(asc(\"txndt\"))))\n",
    "display(sk_orderjoinedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc0f261f-980a-4bd0-bab5-5809f4209439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--CTE Query - Common Table Expression example\n",
    "WITH lag_lead_calculated AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        LAG(amt, 1, -1) OVER (PARTITION BY custid ORDER BY txndt ASC) AS lag_amt,\n",
    "        LEAD(amt, 1, -1) OVER (PARTITION BY custid ORDER BY txndt ASC) AS lead_amt\n",
    "    FROM orderjoineddftv\n",
    ")\n",
    "SELECT \n",
    "    *,\n",
    "    CASE \n",
    "        WHEN lag_amt = -1 THEN 'first transaction'\n",
    "        WHEN amt > lag_amt THEN 'increase'\n",
    "        WHEN amt < lag_amt THEN 'decrease'\n",
    "        ELSE 'same'\n",
    "    END AS transpattern\n",
    "FROM lag_lead_calculated\n",
    "union \n",
    "SELECT \n",
    "    *,\n",
    "    CASE \n",
    "        WHEN lag_amt = -1 THEN 'first transaction'\n",
    "        WHEN amt > lag_amt THEN 'increase'\n",
    "        WHEN amt < lag_amt THEN 'decrease'\n",
    "        ELSE 'same'\n",
    "    END AS transpattern\n",
    "FROM lag_lead_calculated\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9396908-0e18-4339-b661-d90866ed28cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--CTE Query - Common Table Expression example\n",
    "WITH lag_lead_calculated AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        LAG(amt, 1, -1) OVER (PARTITION BY custid ORDER BY txndt ASC) AS priortransamt,\n",
    "        LEAD(amt, 1, -1) OVER (PARTITION BY custid ORDER BY txndt ASC) AS nexttransamt\n",
    "    FROM orderjoineddftv\n",
    ")\n",
    "SELECT \n",
    "    *,\n",
    "    CASE \n",
    "        WHEN priortransamt = -1 THEN 'first transaction'\n",
    "        WHEN amt > priortransamt THEN 'increase'\n",
    "        WHEN amt < priortransamt THEN 'decrease'\n",
    "        ELSE 'same'\n",
    "    END AS transpattern\n",
    "FROM lag_lead_calculated\n",
    "union \n",
    "SELECT \n",
    "    *,\n",
    "    CASE \n",
    "        WHEN priortransamt < 10 THEN 'low transaction'\n",
    "        WHEN priortransamt > 10 THEN 'high transaction'\n",
    "    END AS transpattern\n",
    "    FROM lag_lead_calculated\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5556ef0-7ba6-483b-be84-0fb1324fa6a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    *,\n",
    "    FIRST_VALUE(amt) OVER (\n",
    "        PARTITION BY custid \n",
    "        ORDER BY txndt ASC\n",
    "    ) AS firsttransvalue,\n",
    "    FIRST_VALUE(amt) OVER (\n",
    "        PARTITION BY custid \n",
    "        ORDER BY txndt DESC\n",
    "    ) AS lasttransvalue,\n",
    "    CUME_DIST() OVER (\n",
    "        PARTITION BY custid \n",
    "        ORDER BY txndt DESC\n",
    "    ) AS cumedistribution\n",
    "FROM orderjoineddftv;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e492f712-f3ce-40ec-b4c2-b502b37cd701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Familiar SQL, easily we can write\n",
    "SELECT \n",
    "    product,\n",
    "    spendby,\n",
    "    SUM(amt) AS sumamt,\n",
    "    AVG(amt) AS avgamt\n",
    "FROM orderjoineddftv\n",
    "GROUP BY product, spendby\n",
    "having sum(amt) > 100\n",
    "ORDER BY \n",
    "    product ASC,\n",
    "    sumamt DESC\n",
    "limit 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84dba286-b6d5-4bef-bd7e-d8b87a7b46ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    product,\n",
    "    spendby,\n",
    "    SUM(amt) AS sumamt,\n",
    "    AVG(amt) AS avgamt\n",
    "    from orderjoineddftv\n",
    "    group by ROLLUP(product,spendby)\n",
    "    order by product,spendby,sumamt DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb55bc89-5838-4452-a916-53849b1f2b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    product,\n",
    "    spendby,\n",
    "    SUM(amt) AS sumamt,\n",
    "    AVG(amt) AS avgamt\n",
    "    from orderjoineddftv\n",
    "    group by CUBE(product,spendby)\n",
    "    order by product,spendby,sumamt DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b24fb87-5941-4fce-aedf-4029d3460b5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Pivot in spark sql is tricky and not much dynamic, hence DSL we can consider, but all other windowing or analytical functions, we can consider SQL over DSL\n",
    "create or replace temp view orderjoineddftvout as\n",
    "SELECT * FROM (\n",
    "  SELECT \n",
    "    state, \n",
    "    city, \n",
    "    category, \n",
    "    amt \n",
    "  FROM orderjoineddftv\n",
    ")\n",
    "PIVOT (\n",
    "  SUM(amt) AS sum_amt, \n",
    "  AVG(amt) AS avg_amt\n",
    "  FOR category IN ('Exercise & Fitness', 'Jumping')\n",
    ")\n",
    "ORDER BY state;\n",
    "select * from orderjoineddftvout;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6708601491883503,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4.logistics_usecase_BB_SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
