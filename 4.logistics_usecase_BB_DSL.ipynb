{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba3eed82-8353-4144-a5e0-7b00cd80a2bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#1. Data Munging -\n",
    "1. Data Munging - (Cleanup) Process of transforming and mapping data from Raw form into Tidy(usable) format with the intent of making it more appropriate and valuable for a variety of downstream purposes such for further Transformation/Enrichment, Egress/Outbound, analytics, Datascience/AI application & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc46130b-4038-4f94-af86-d0bb5a6bbbd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We have to define Spark Session to enter into Spark application\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Spark DataFrames bread and butter\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6cc77ca-2fdc-445f-ac04-49ea7515e8e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Data Exploration programatically\n",
    "\n",
    "rawdf=spark.read.csv(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_source1\",header=True,inferSchema=True).toDF(\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\")\n",
    "display(rawdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc71800d-68db-4b2c-85af-b716043079cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive Munging - EDA of schema/structure functions we can use\n",
    "rawdf.printSchema()\n",
    "print(rawdf.schema)\n",
    "print(rawdf.dtypes)\n",
    "print(rawdf.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7049a36-0146-4c58-a305-2477c47c2154",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive Datamunging - EDA data functions we can use\n",
    "#We identified few patterns on this data\n",
    "#1. Deduplication of rows and given column(s)\n",
    "#2. Null values ratio across all columns\n",
    "#3. Distribution (Dense) of the data across all number columns\n",
    "#4. Min, Max values\n",
    "#5. StdDeviation - Standard deviation tells you how much the data varies from the average (mean).\n",
    "#6. Percentile - Distribution percentage from 0 to 100 in 4 quadrants of 25%\n",
    "print(\"actual count of the data\",rawdf.count())\n",
    "print(\"de-duplicated record (all columns) count\",rawdf.distinct().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count\",rawdf.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given cid column count\",rawdf.dropDuplicates(['shipment_id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "display(rawdf.describe())\n",
    "display(rawdf.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6adf0339-5eed-4d12-afc4-2ec656e86ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Data Munging - (File: logistics_source1 and logistics_source2)\n",
    "Without modifying the data, identify:\n",
    "Shipment IDs that appear in both master_v1 and master_v2\n",
    "Records where:\n",
    "\n",
    "shipment_id is non-numeric\n",
    "age is not an integer\n",
    "Count rows having: 3. fewer columns than expected 4. more columns than expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67d7e500-eda9-4f5e-bef4-a68c5c076a06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging** File: logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11fc7075-d126-4020-9b33-89c1bd0a36d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Combining Data + Schema Merging (Structuring)\n",
    "1. Read both files without enforcing schema\n",
    "2. Align them into a single canonical schema: shipment_id,\n",
    "first_name,\n",
    "last_name,\n",
    "age,\n",
    "role,\n",
    "hub_location,\n",
    "vehicle_type,\n",
    "data_source\n",
    "3. Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23d5766e-09ee-4f36-9ae4-31f2efbe206d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768364219447}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768540484012}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extraction (Ingestion) methodologies\n",
    "from pyspark.sql.functions import lit,initcap\n",
    "\n",
    "struct1=\"shipment_id string ,first_name string ,last_name string ,age string ,role string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_source1\",header=True,inferSchema=True).withColumn(\"sourcesystem1\",lit(\"system1\"))\n",
    "display(rawdf1)\n",
    "rawdf2=spark.read.csv(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_source2\",header=True,inferSchema=True).withColumn(\"sourcesystem2\",lit(\"system2\"))\n",
    "\n",
    "\n",
    "#display(rawdf2)\n",
    "rawdf1.write.parquet(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_source1.parquet\",mode='overwrite')\n",
    "rawdf2.write.parquet(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_source1.parquet\",mode=\"append\")\n",
    "merge_df=spark.read.parquet(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_source1.parquet\",mergeSchema=True)\n",
    "display(merge_df)\n",
    "#Transformation (Munging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09a39d00-656a-46d5-9d7e-de0a5317b6ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Cleansing, Scrubbing: \n",
    "Cleansing (removal of unwanted datasets)<br>\n",
    "1. Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role<br>\n",
    "2. Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name<br>\n",
    "3. Join Readiness Rule - Drop records where the join key is null: shipment_id<br>\n",
    "\n",
    "Scrubbing (convert raw to tidy)<br>\n",
    "4. Age Defaulting Rule - Fill NULL values in the age column with: -1<br>\n",
    "5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN<br>\n",
    "6. Invalid Age Replacement - Replace the following values in age:\n",
    "\"ten\" to -1\n",
    "\"\" to -1<br>\n",
    "7. Vehicle Type Normalization - Replace inconsistent vehicle types: \n",
    "truck to LMV\n",
    "bike to TwoWheeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6f098a-b5db-4ba2-a93a-e06f1bd24871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "struct1=StructType([StructField(\"shipment_id\",StringType(),True),\n",
    "                    StructField(\"first_name\",StringType(),True),\n",
    "                    StructField(\"last_name\",StringType(),True),\n",
    "                    StructField(\"age\",StringType(),True),\n",
    "                    StructField(\"role\",StringType(),True)\n",
    "                   ])\n",
    "df_method1=spark.read.schema(struct1).csv(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_source1\",mode='permissive')\n",
    "#display(df_method1)\n",
    "print(\"entire count of data\",df_method1.count())\n",
    "print(\"after scrubbing, count of data\",len(df_method1.collect()))\n",
    "\n",
    "#method drop_malformed\n",
    "\n",
    "dfmethod2=spark.read.schema(struct1).csv(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_source1\",mode='dropMalformed')\n",
    "#display(dfmethod2)\n",
    "print(\"entire count of data\",dfmethod2.count())\n",
    "print(\"after scrubbing, count of data\",len(dfmethod2.collect()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b122d15-3775-4051-94c6-2390ea6b3566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#method3 best methodology of applying active data munging\n",
    "dfmethiod3=spark.read.schema(struct1).csv(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_source1\",mode='Permissive')\n",
    "#display(dfmethiod3)\n",
    "print(\"entire count of data\",dfmethiod3.count())\n",
    "print(\"after scrubbing, count of data\",len(dfmethiod3.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6b3f04d-2e6d-432e-a4f1-c1767d5f889a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Before actively Cleansing or Scrubbing - We have to create a Rejection Strategy to reduce data challenges in the future\n",
    "struct11=StructType([StructField(\"shipment_id\",StringType(),True),\n",
    "                    StructField(\"first_name\",StringType(),True),\n",
    "                    StructField(\"last_name\",StringType(),True),\n",
    "                    StructField(\"age\",StringType(),True),\n",
    "                    StructField(\"role\",StringType(),True),\n",
    "                    StructField(\"corruptdata\",StringType(),True)\n",
    "                   ])\n",
    "dfmethiod3=spark.read.schema(struct11).csv(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_source1\",mode='permissive',columnNameOfCorruptRecord=\"corruptdata\")\n",
    "#display(dfmethiod3)\n",
    "print(\"entire count of data\",dfmethiod3.count())\n",
    "print(\"after scrubbing, count of data\",len(dfmethiod3.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ce94cf4-6760-4e7f-bdc3-0ee023445ffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We already know how to do cleansing applying the strict Structure on method1 and method2, but we can do cleansing and scrubbing in a controlled fashion by applying functions on the method3 dataframe\n",
    "#Important na functions we can use to do cleansing\n",
    "struct11=StructType([StructField(\"shipment_id\",StringType(),True),\n",
    "                    StructField(\"first_name\",StringType(),True),\n",
    "                    StructField(\"last_name\",StringType(),True),\n",
    "                    StructField(\"age\",StringType(),True)    ,\n",
    "                    StructField(\"role\",StringType(),True)\n",
    "                   ])\n",
    "\n",
    "clenaseddf=spark.read.schema(struct11).csv(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_source1\",mode='permissive',header='True')\n",
    "\n",
    "print(\"entire count of data\",clenaseddf.count())\n",
    "print(\"after scrubbing cleansed count of data\",len(clenaseddf.collect()))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5728de09-07d0-4ce9-9b6d-3bc955fc2797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clansed_df1=clenaseddf.na.drop(\"any\")\n",
    "clansed_df1=clenaseddf.na.drop(\"any\",subset=[\"shipment_id\",\"role\"])\n",
    "#display(clansed_df1.take(10))\n",
    "print(\"entire count of data\",clansed_df1.count())\n",
    "print(\"after scrubbing cleansed count of data\",len(clansed_df1.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25933187-5ba5-40b3-a1c0-b65d12de802d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name\n",
    "clen_df2=clansed_df1.na.drop(\"all\",subset=[\"first_name\",\"last_name\"])\n",
    "#display(clen_df2.take(10))\n",
    "print(\"entire count of data\",clen_df2.count())\n",
    "print(\"after scrubbing cleansed count of data\",len(clen_df2.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "382a4c0e-c7fa-4d68-85a3-134dac9e597d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Join Readiness Rule - Drop records where the join key is null: shipment_id\n",
    "clen_df3=clen_df2.na.drop(\"any\",subset=[\"shipment_id\"])\n",
    "#display(clen_df3.take(10))\n",
    "print(\"entire count of data\",clen_df3.count())\n",
    "print(\"after scrubbing cleansed count of data\",len(clen_df3.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b8674c8-db6c-4ac7-bf0b-c5d10f2687df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Scrubbing: \n",
    "\n",
    "\n",
    "Scrubbing (convert raw to tidy)<br>\n",
    "4. Age Defaulting Rule - Fill NULL values in the age column with: -1<br>\n",
    "5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN<br>\n",
    "6. Invalid Age Replacement - Replace the following values in age:\n",
    "\"ten\" to -1\n",
    "\"\" to -1<br>\n",
    "7. Vehicle Type Normalization - Replace inconsistent vehicle types: \n",
    "truck to LMV\n",
    "bike to TwoWheeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccdb6198-b7c4-4693-9e29-e50f986b9021",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "clen_df4=clen_df3.na.fill(value= -1,subset=[\"age\"])\n",
    "#display(clen_df4.take(10))\n",
    "print(\"Total number of rows after applying the age defaulting rule: \",clen_df4.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d588dc81-2b47-4f96-8538-4517c1ade457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf2=spark.read.csv(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_source2\",header='True')\n",
    "display(rawdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af3ad8f-1df6-4792-af1a-bd72a2b4ca3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merge_df1=merge_df.na.drop(\"any\",subset=[\"hub_location\",\"vehicle_type\"])\n",
    "merge_df1=merge_df1.na.drop(\"any\",subset=[\"shipment_id\"])\n",
    "merge_df1=merge_df1.na.drop(\"all\",subset=[\"first_name\",\"last_name\"])\n",
    "merge_df1=merge_df1.na.fill(value= -1,subset=[\"age\"]).na.fill(value=\"unknown\",subset=[\"vehicle_type\"])\n",
    "#display(merge_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db4cf488-89da-40aa-bede-52c84e2ea5cd",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768665306790}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Cell 22"
    }
   },
   "outputs": [],
   "source": [
    "#Invalid Age Replacement - Replace the following values in age: \"ten\":-1,\"\" to \"-1\"\n",
    "merge_df2=merge_df1.na.replace({'ten': '-1', '': '-1'}, subset=[\"shipment_id\",\"age\"])\n",
    "merge_df2=merge_df2.na.replace({'': '-1'}, subset=[\"shipment_id\",\"age\"])\n",
    "display(merge_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e3d5b26-0230-48f3-9921-608a120c7013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Vehicle Type Normalization - Replace inconsistent vehicle types: truck to LMV bike to TwoWheeler\n",
    "merge_df3=merge_df2.na.replace({\"Truck\":\"LMV\",\"Bike\":\"TwoWheeler\"},subset=[\"vehicle_type\"])\n",
    "#display(merge_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e8ab454-7944-45d8-be4a-2e7e67f9f26f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Column Uniformity: role - Convert to lowercase\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "#display(merge_df3)\n",
    "merge_df4=merge_df3.withColumn(\"role\",lower(col(\"role\")))\n",
    "display(merge_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39e57efc-df64-486e-8b38-d8b82b70b0b3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 24"
    }
   },
   "outputs": [],
   "source": [
    "#Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#vehicle_type - Convert values to UPPERCASE\n",
    "merge_df4=merge_df4.withColumn(\"vehicle_type\",upper(col(\"vehicle_type\")))\n",
    "\n",
    "#Source Files: hub_location - Convert values to initcap case\n",
    "merge_df4=merge_df4.withColumn(\"hub_location\",initcap(col(\"hub_location\")))\n",
    "#display(merge_df4)\n",
    "\n",
    "#Source Files: DF of merged(logistics_source1 & logistics_source2)\n",
    "merge_df4=merge_df4.withColumnRenamed(\"sourcesystem1\",\"logistics_source1\")\n",
    "merge_df4=merge_df4.withColumnRenamed(\"sourcesystem2\",\"logistics_source2\")\n",
    "#display(merge_df4)\n",
    "#Standardizing column data types to fix schema drift and enable mathematical operations.\n",
    "#Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "#age: Cast String to Integer\n",
    "#print(merge_df4.printSchema())\n",
    "merge_df4=merge_df4.withColumn(\"age\",col(\"age\").cast(IntegerType()))\n",
    "#print(merge_df4.printSchema())\n",
    "\n",
    "#Rename: first_name to staff_first_name\n",
    "#Rename: last_name to staff_last_name\n",
    "#Rename: hub_location to origin_hub_city\n",
    "merge_df5=merge_df4.withColumnRenamed(\"first_name\",\"staff_first_name\").withColumnRenamed(\"last_name\",\"staff_last_name\").withColumnRenamed(\"hub_location\",\"origin_hub_city\")\n",
    "#display(merge_df5)\n",
    "#shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)\n",
    "\n",
    "merge_df6=merge_df5.select(\"shipment_id\",\"staff_first_name\",\"staff_last_name\",\"role\",\"origin_hub_city\")\n",
    "\n",
    "#Apply Record Level De-Duplication\n",
    "#Apply Column Level De-Duplication (Primary Key Enforcement)\n",
    "\n",
    "merge_df6=merge_df6.distinct()\n",
    "merge_df6=merge_df6.dropDuplicates()\n",
    "merge_df6=merge_df6.dropDuplicates([\"shipment_id\"])\n",
    "merge_df=merge_df6.filter(col(\"shipment_id\") !=\"ten\")\n",
    "display(merge_df6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8ce7f4c-4ac1-4b83-8bb4-6ec1ed503b1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd69685f-05fa-43be-8ec2-f3bd66ce5106",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768467909944}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Column Uniformity: role - Convert to lowercase\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "#display(merge_df3)\n",
    "merge_df4=merge_df3.withColumn(\"role\",lower(col(\"role\")))\n",
    "display(merge_df4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d18b1d2-3c46-4371-af5a-0d8cd7fcef59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 26"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "jsondf1 = spark.read \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .json(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_shipment_detail_3000.json\")\n",
    "\n",
    "#display(jsondf1)\n",
    "jsondf2=jsondf1.withColumn(\"domain\",lit(\"Logistics\")).withColumn(\"ingestion_timestamp\",current_timestamp()).withColumn(\"is_expedited\",lit(False))\n",
    "display(jsondf2.take(10))\n",
    "#Format Standardization:\n",
    "#Source Files: DF of logistics_shipment_detail_3000.json\n",
    "#Convert shipment_date to yyyy-MM-dd\n",
    "jsondf3=jsondf2.withColumn(\"shipment_date\",to_date(col(\"shipment_date\"),\"yy-MM-dd\"))\n",
    "#Ensure shipment_cost has 2 decimal precision\n",
    "jsondf3=jsondf3.withColumn(\"shipment_cost\",round(col(\"shipment_cost\"),2))\n",
    "display(jsondf3.take(10))\n",
    "#Source File: DF of logistics_shipment_detail_3000.json\n",
    "#shipment_weight_kg: Cast to Double\n",
    "#print(jsondf3.printSchema())\n",
    "jsondf4=jsondf3.withColumn(\"shipment_weight_kg\",col(\"shipment_weight_kg\").cast(DoubleType()))\n",
    "#is_expedited: Cast to Boolean\n",
    "jsondf4=jsondf4.withColumn(\"is_expedited\",col(\"is_expedited\").cast(BooleanType()))\n",
    "#print(jsondf4.printSchema())\n",
    "\n",
    "#Apply Record Level De-Duplication\n",
    "#Apply Column Level De-Duplication (Primary Key Enforcement)\n",
    "jsondf4=jsondf4.distinct()\n",
    "jsondf4_dedup=jsondf4.dropDuplicates()\n",
    "#display(jsondf4_dedup)\n",
    "#shipment_cost (Metric), ingestion_timestamp (Audit)\n",
    "jsondf4_select=jsondf4.select(\"shipment_cost\",\"ingestion_timestamp\")\n",
    "#display(jsondf4_select)\n",
    "jsondf4_select.distinct()\n",
    "jsondf4_select.dropDuplicates()\n",
    "jsondf4_select.dropDuplicates([\"shipment_cost\"])\n",
    "display(jsondf4_select)\n",
    "\n",
    "                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f65a884-9b4e-45d6-ba19-cc04de0efe30",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768381221574}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#vehicle_type - Convert values to UPPERCASE\n",
    "merge_df4=merge_df4.withColumn(\"vehicle_type\",upper(col(\"vehicle_type\")))\n",
    "\n",
    "\n",
    "#Source Files: hub_location - Convert values to initcap case\n",
    "\n",
    "merge_df4=merge_df4.withColumn(\"hub_location\",initcap(col(\"hub_location\")))\n",
    "#display(merge_df4)\n",
    "\n",
    "#Source Files: DF of merged(logistics_source1 & logistics_source2)\n",
    "merge_df4=merge_df4.withColumnRenamed(\"sourcesystem1\",\"logistics_source1\")\n",
    "merge_df4=merge_df4.withColumnRenamed(\"sourcesystem2\",\"logistics_source2\")\n",
    "#display(merge_df4)\n",
    "#Standardizing column data types to fix schema drift and enable mathematical operations.\n",
    "#Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "#age: Cast String to Integer\n",
    "print(merge_df4.printSchema())\n",
    "merge_df4=merge_df4.withColumn(\"age\",col(\"age\").cast(IntegerType()))\n",
    "print(merge_df4.printSchema())\n",
    "\n",
    "#Rename: first_name to staff_first_name\n",
    "#Rename: last_name to staff_last_name\n",
    "#Rename: hub_location to origin_hub_city\n",
    "merge_df5=merge_df4.withColumnRenamed(\"first_name\",\"staff_first_name\").withColumnRenamed(\"last_name\",\"staff_last_name\").withColumnRenamed(\"hub_location\",\"origin_hub_city\")\n",
    "display(merge_df5)\n",
    "#shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)\n",
    "\n",
    "\n",
    "merge_df6=merge_df5.select(\"shipment_id\",\"staff_first_name\",\"staff_last_name\",\"role\",\"origin_hub_city\")\n",
    "\n",
    "#Apply Record Level De-Duplication\n",
    "#Apply Column Level De-Duplication (Primary Key Enforcement)\n",
    "merge_df6=merge_df6.distinct()\n",
    "merge_df6=merge_df6.dropDuplicates()\n",
    "merge_df6=merge_df6.dropDuplicates([\"shipment_id\"])\n",
    "display(merge_df6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d99dfb0a-2c8a-4dff-8ee4-830910db07fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Data Enrichment - Detailing of data\n",
    "Makes your data rich and detailed <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb52f65-c44d-45f4-a112-80dac2cd054f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Add Audit Timestamp (load_dt) Source File: DF of logistics_source1 and logistics_source2\n",
    "#display(merge_df)\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "merge_df7=merge_df6.withColumn(\"load_dt\",current_timestamp())\n",
    "#display(merge_df7)\n",
    "\n",
    "#Create Full Name (full_name) Source File: DF of logistics_source1 and logistics_source2\n",
    "merge_df8=merge_df7.withColumn(\"full_name\",concat(col(\"staff_first_name\"),lit(\" \"),col(\"staff_last_name\")))\n",
    "#display(merge_df8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "729eadaf-79ca-4d64-83ff-27262127ffc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Define Route Segment (route_segment) Source File: DF of logistics_shipment_detail_3000.json\n",
    "#Scenario: The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    "#Action: Combine source_city and destination_city with a hyphen.\n",
    "#Result: \"Chennai\" + \"-\" + \"Pune\" -> \"Chennai-Pune\"\n",
    "\n",
    "#display(jsondf4)\n",
    "jsondf5=jsondf4.withColumn(\"route_segment\",concat(col(\"source_city\"),lit(\"-\"),col(\"destination_city\")))\n",
    "#display(jsondf5)\n",
    "\n",
    "#4. Generate Vehicle Identifier (vehicle_identifier) Source File: DF of logistics_shipment_detail_3000.json\n",
    "\n",
    "#Scenario: We need a unique tracking code that immediately tells us the vehicle type and the shipment ID.\n",
    "#Action: Combine vehicle_type and shipment_id to create a composite key.\n",
    "#Result: \"Truck\" + \"_\" + \"500001\" -> \"Truck_500001\"\n",
    "\n",
    "jsondf6=jsondf5.withColumn(\"vehicle_identifier\",concat(col(\"vehicle_type\"),lit(\"_\"),col(\"shipment_id\")))\n",
    "display(jsondf6)\n",
    "#5. Generate Shipment Identifier (shipment_identifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd0df73c-02ca-4b1e-a929-aa966239b3c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Deriving of Columns (Time Intelligence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffba5817-4d39-4325-a011-7a795c21138a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Derive Shipment Year (shipment_year)\n",
    "\n",
    "#Extracting temporal features from dates to enable period-based analysis and reporting.\n",
    "#Source File: logistics_shipment_detail_3000.json\n",
    "\n",
    "#Scenario: Management needs an annual performance report to compare growth year-over-year.\n",
    "#Action: Extract the year component from shipment_date.\n",
    "#Result: \"2024-04-23\" -> 2024\n",
    "jsondf7=jsondf6.withColumn(\"shipment_year\",year(col(\"shipment_date\").cast(DateType())))\n",
    "\n",
    "#2. Derive Shipment Month (shipment_month)\n",
    "\n",
    "jsondf7=jsondf7.withColumn(\"shipment_month\",month(col(\"shipment_date\")))\n",
    "\n",
    "#3. Flag Weekend Operations (is_weekend)\n",
    "\n",
    "#Scenario: The Operations team needs to track shipments handled during weekends to calculate overtime pay or analyze non-business day capacity.\n",
    "#Action: Flag as 'True' if the shipment_date falls on a Saturday or Sunday.\n",
    "jsondf7=jsondf7.withColumn(\"is_weekend\",when((dayofweek(col(\"shipment_date\"))==1)|(dayofweek(col(\"shipment_date\"))==7),\"True\").otherwise(\"False\"))\n",
    "display(jsondf7)\n",
    "\n",
    "\n",
    "#4. Flag shipment status (is_expedited)\n",
    "\n",
    "#Scenario: The Operations team needs to track shipments is IN_TRANSIT or DELIVERED.\n",
    "#Action: Flag as 'True' if the shipment_status IN_TRANSIT or DELIVERED.\n",
    "jsondf7=jsondf7.withColumn(\"is_expedited\",when((col(\"shipment_status\")==\"IN_TRANSIT\")|(col(\"shipment_status\")==\"DELIVERED\"),\"True\").otherwise(\"False\"))\n",
    "display(jsondf7)\n",
    "#1. Derive Shipment Year (shipment_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85941610-8a90-4d4b-9c7c-12e1bb75bddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Enrichment/Business Logics (Calculated Fields)\n",
    "Deriving new metrics and financial indicators using mathematical and date-based operations.\n",
    "Source File: logistics_shipment_detail_3000.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f386dfd3-7e9d-4280-be10-57040ed8f2d2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Subtotal report by hub and vehicle type (fix alias typo)"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d95d8388-8a7d-4c3a-b41e-3a461120ba78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#. Calculate Unit Cost (cost_per_kg)\n",
    "\n",
    "#Scenario: The Finance team wants to analyze the efficiency of shipments by determining the cost incurred per unit of weight.\n",
    "#Action: Divide shipment_cost by shipment_weight_kg.\n",
    "#Logic: shipment_cost / shipment_weight_kg\n",
    "\n",
    "jsondf8=jsondf7.withColumn(\"cost_per_kg\",col(\"shipment_cost\")/col(\"shipment_weight_kg\"))\n",
    "#display(jsondf8)\n",
    "\n",
    "#2. Track Shipment Age (days_since_shipment)\n",
    "\n",
    "#Scenario: The Operations team needs to monitor how long it has been since a shipment was dispatched to identify potential delays.\n",
    "#Action: Calculate the difference in days between the current_date and the shipment_date.\n",
    "#Logic: datediff(current_date(), shipment_date)\n",
    "\n",
    "jsondf8=jsondf8.withColumn(\"days_since_shipment\",datediff(current_date(),col(\"shipment_date\")))\n",
    "display(jsondf8)\n",
    "\n",
    "\n",
    "#3. Compute Tax Liability (tax_amount)\n",
    "\n",
    "#Scenario: For invoicing and compliance, we must calculate the Goods and Services Tax (GST) applicable to each shipment.\n",
    "#Action: Calculate 18% GST on the total shipment_cost.\n",
    "#Logic: shipment_cost * 0.18\n",
    "\n",
    "jsondf9=jsondf8.withColumn(\"tax_amount\",col(\"shipment_cost\")*0.18)\n",
    "display(jsondf9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1257ea6b-4394-4bac-bff6-c90c38e7806a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Remove/Eliminate (drop, select, selectExpr)\n",
    "Excluding unnecessary or redundant columns to optimize storage and privacy.\n",
    "Source File: DF of logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0abbc2e-390c-4d96-bd49-ec88bbf8ee5e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768395243245}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#. Remove Redundant Name Columns\n",
    "\n",
    "#Scenario: Since we have already created the full_name column in the Enrichment step, the individual name columns are now redundant and clutter the dataset.\n",
    "#Action: Drop the first_name and last_name columns.\n",
    "#Logic: df.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "merge_df8=merge_df8.drop(\"staff_first_name\", \"staff_last_name\")\n",
    "display(merge_df8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f18343-c77e-4de3-bb54-69b7b19f29cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Select, Filter, Derive Flags"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02fa32e1-ad87-4575-ae13-5cb71419803e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fix currency formatting syntax error"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666c6d4f-bf39-4134-a6eb-d31efad4b75a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fix format_number datatype error"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df515ca-7f12-4963-b5db-2f481e286ba2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Splitting & Merging/Melting of Columns\n",
    "Reshaping columns to extract hidden values or combine fields for better analysis.\n",
    "Source File: DF of logistics_shipment_detail_3000.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f25cb181-5627-40d2-bbd1-0f162212b8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Splitting (Extraction) Breaking one column into multiple to isolate key information.\n",
    "display(jsondf9)\n",
    "#Split Order Code:\n",
    "#Action: Split order_id (\"ORD100000\") into two new columns:\n",
    "#order_prefix (\"ORD\")\n",
    "#order_sequence (\"100000\")\n",
    "jsondf10=jsondf9.withColumn(\"order_prefix\",regexp_extract('order_id','([A-Z]+)',1))\n",
    "jsondf10=jsondf9.withColumn(\"order_sequence\",regexp_extract('order_id','([0-9]+)',1))\n",
    "display(jsondf10)\n",
    "\n",
    "#Split Date:\n",
    "#Action: Split shipment_date into three separate columns for partitioning:\n",
    "#ship_year (2024)\n",
    "#ship_month (4)\n",
    "#ship_day (23)\n",
    "\n",
    "jsondf11=jsondf10.withColumn(\"ship_year\",year(col(\"shipment_date\")))\n",
    "jsondf11=jsondf11.withColumn(\"ship_month\",month(col(\"shipment_date\")))\n",
    "jsondf11=jsondf11.withColumn(\"ship_day\",dayofmonth(col(\"shipment_date\")))\n",
    "\n",
    "display(jsondf11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddbd5479-847d-4feb-90e9-683c42d0e0c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Merging (Concatenation) Combining multiple columns into a single unique identifier or description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1200092e-70f9-4135-a9ba-d89d1c1d7b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create Route ID:\n",
    "#Action: Merge source_city (\"Chennai\") and destination_city (\"Pune\") to create a descriptive route key:\n",
    "#route_lane (\"Chennai->Pune\")\n",
    "jsondf11=jsondf11.withColumn(\"route_lane\",concat(col(\"source_city\"),lit(\"->\"),col(\"destination_city\")))\n",
    "display(jsondf11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e87bd4d-d93a-4be3-8948-69790b1aa83d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Group and aggregate shipment cost by hub and vehicle type"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fbde418-174c-47a4-8830-f1a1b24cb88f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fix CAST_INVALID_INPUT error in windowed datediff"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, lag, col, try_to_date\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_df = spark.read.json(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_shipment_detail_3000.json\", multiLine=True)\n",
    "display(window_df)\n",
    "\n",
    "# Use try_to_date to convert shipment_date to DATE\n",
    "window_df = window_df.withColumn(\"shipment_date\", try_to_date(col(\"shipment_date\"), \"yy-MM-dd\"))\n",
    "\n",
    "lagdf = window_df.withColumn(\n",
    "    \"prev_shipment_date\",\n",
    "    lag(\"shipment_date\", 1).over(Window.partitionBy(col(\"vehicle_type\")).orderBy(col(\"shipment_date\")))\n",
    ")\n",
    "lagdf = lagdf.withColumn(\n",
    "    \"days_elapsed\",\n",
    "    datediff(col(\"shipment_date\"), col(\"prev_shipment_date\"))\n",
    ")\n",
    "lagdf = lagdf.drop(\"prev_shipment_date\")\n",
    "display(lagdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1929870-46d9-4db4-81ab-e523f4594639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Data Customization & Processing - Application of Tailored Business Specific Rules\n",
    "\n",
    "### **UDF1: Complex Incentive Calculation**\n",
    "**Scenario:** The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "\n",
    "**Action:** Create a Python function `calculate_bonus(role, age)` and register it as a Spark UDF.\n",
    "\n",
    "**Logic:**\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` > 50:\n",
    "  * `Bonus` = 15% of Salary (Reward for Seniority)\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` < 30:\n",
    "  * `Bonus` = 5% of Salary (Encouragement for Juniors)\n",
    "* **ELSE**:\n",
    "  * `Bonus` = 0\n",
    "\n",
    "**Result:** A new derived column `projected_bonus` is generated for every row in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **UDF2: PII Masking (Privacy Compliance)**\n",
    "**Scenario:** For the analytics dashboard, we must hide the full identity of the staff to comply with privacy laws (GDPR/DPDP), while keeping names recognizable for internal managers.\n",
    "\n",
    "**Business Rule:** Show the first 2 letters, mask the middle characters with `****`, and show the last letter.\n",
    "\n",
    "**Action:** Create a UDF `mask_identity(name)`.\n",
    "\n",
    "**Example:**\n",
    "* **Input:** `\"Rajesh\"`\n",
    "* **Output:** `\"Ra****h\"`\n",
    "<br>\n",
    "**Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1745229b-22a3-4e30-85b6-14e6c0774969",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768447010387}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Scenario: The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "\n",
    "#Action: Create a Python function calculate_bonus(role, age) and register it as a Spark UDF.\n",
    "\n",
    "#Logic:\n",
    "\n",
    "#IF Role == 'Driver' AND Age > 50:\n",
    "#Bonus = 15% of Salary (Reward for Seniority)\n",
    "#IF Role == 'Driver' AND Age < 30:\n",
    "#Bonus = 5% of Salary (Encouragement for Juniors)\n",
    "#ELSE:\n",
    "#Bonus = 0\n",
    "#Result: A new derived column projected_bonus is generated for every row in the dataset.\n",
    "#display(merge_df5)\n",
    "#Lets Create an UDF to calculate agecateogy of customers\n",
    "#1. Import udf library\n",
    "from pyspark.sql.functions import udf\n",
    "#2. Create a Python function - Can only work in single local computer\n",
    "def calculate_bonus(role, age):\n",
    "    if role == 'driver' and age > 50:\n",
    "        return 0.15\n",
    "    elif role == 'driver' and age < 30:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0.0\n",
    "#3. Register the Python function as a Spark UDF\n",
    "bonus_udf = udf(calculate_bonus)\n",
    "#4. Apply the UDF to the DataFrame\n",
    "jsondf11_udf = merge_df5.withColumn(\"projected_bonus\", bonus_udf(col(\"role\"), col(\"age\")))\n",
    "display(jsondf11_udf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61f9cec0-a921-4f48-84f2-769690099f7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####UDF2: PII Masking (Privacy Compliance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c507cc6a-e955-436d-96b1-911f57629bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Scenario: For the analytics dashboard, we must hide the full identity of the staff to comply with privacy laws (GDPR/DPDP), while keeping names recognizable for internal managers.\n",
    "\n",
    "#Business Rule: Show the first 2 letters, mask the middle characters with ****, and show the last letter.\n",
    "\n",
    "#Action: Create a UDF mask_identity(name).\n",
    "\n",
    "#Example:\n",
    "\n",
    "#Input: \"Rajesh\"\n",
    "#Output: \"Ra****h\"\n",
    "#**Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "def mask_identity(name):\n",
    "    if name is None:\n",
    "        return None\n",
    "    return name[:2] + '*' * (len(name) - 4) + name[-1]\n",
    "mask_identity_udf = udf(mask_identity)\n",
    "udf_df=merge_df5.withColumn(\"staff_first_name\",mask_identity_udf(col(\"staff_last_name\")))\n",
    "display(udf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9551bd-310b-482f-9eb3-8d404c91ea78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Data Core Curation & Processing (Pre-Wrangling)\n",
    "*Applying business logic to focus, filter, and summarize data before final analysis.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d280c86-7588-4bd2-9324-e28043928385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1. Select (Projection)\n",
    "Source Files: DF of logistics_source1 and logistics_source2\n",
    "\n",
    "Scenario: The Driver App team only needs location data, not sensitive HR info.\n",
    "Action: Select only first_name, role, and hub_location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ca3c19-112b-4006-b425-daa33cbd0ff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Select (Projection)\n",
    "#Source Files: DF of logistics_source1 and logistics_source2\n",
    "\n",
    "#Scenario: The Driver App team only needs location data, not sensitive HR info.\n",
    "#Action: Select only first_name, role, and hub_location.\n",
    "display(merge_df4)\n",
    "predf=merge_df4.select(\"first_name\",\"role\",\"hub_location\")\n",
    "display(predf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22750b33-f831-4aa3-9999-5f1751ca6412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Filter (Selection)\n",
    "#Source File: DF of json\n",
    "\n",
    "#Scenario: We need a report on active operational problems.\n",
    "#Action: Filter rows where shipment_status is 'DELAYED' or 'RETURNED'.\n",
    "#display(jsondf11)\n",
    "\n",
    "predfjson=jsondf11.filter((col(\"shipment_status\") == \"DELAYED\") | (col(\"shipment_status\") == \"RETURNED\"))\n",
    "#display(predfjson)\n",
    "\n",
    "#Scenario: Insurance audit for senior staff.\n",
    "#Action: Filter rows where age > 50.\n",
    "predf1=merge_df4.filter(col(\"age\") > 50)\n",
    "#display(predf1)\n",
    "\n",
    "#3. Derive Flags & Columns (Business Logic)\n",
    "#Source File: DF of json\n",
    "\n",
    "#Scenario: Identify high-value shipments for security tracking.\n",
    "#Action: Create flag is_high_value = True if shipment_cost > 50,000.\n",
    "\n",
    "predfjson1=predfjson.withColumn(\"is_high_value\",when(col(\"shipment_cost\") > 50000,\"True\").otherwise(\"False\"))\n",
    "#display(predfjson1)\n",
    "\n",
    "#Scenario: Flag weekend operations for overtime calculation.\n",
    "#Action: Create flag is_weekend = True if day is Saturday or Sunday.\n",
    "predfjson1=predfjson1.withColumn(\"is_weekend\",when((dayofweek(col(\"shipment_date\"))==1)|(dayofweek(col(\"shipment_date\"))==7),\"True\").otherwise(\"False\"))\n",
    "display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16bd27d6-5250-470f-90b7-04f72ac0c61f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###4. Format (Standardization)\n",
    "Source File: DF of json\n",
    "\n",
    "Scenario: Finance requires readable currency formats.\n",
    "Action: Format shipment_cost to string like \"30,695.80\".\n",
    "Scenario: Standardize city names for reporting.\n",
    "Action: Format source_city to Uppercase (e.g., \"chennai\"  \"CHENNAI\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e01fe53-080c-4dd4-a42b-fdd66dbdcc29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4. Format (Standardization)\n",
    "#Source File: DF of json\n",
    "\n",
    "#Scenario: Finance requires readable currency formats.\n",
    "#Action: Format shipment_cost to string like \"30,695.80\".\n",
    "\n",
    "display(predfjson1)\n",
    "predfjson1.printSchema()\n",
    "predfjson1=predfjson1.withColumn(\"shipment_cost\",format_number(col(\"shipment_cost\"),2))\n",
    "\n",
    "display(predfjson1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85f13dad-e32e-4d1f-91db-5155f7e991b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Scenario: Standardize city names for reporting.\n",
    "#Action: Format source_city to Uppercase (e.g., \"chennai\"  \"CHENNAI\").\n",
    "\n",
    "predfjson2=predfjson1.withColumn(\"source_city\",upper(col(\"source_city\")))\n",
    "display(predfjson2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75dd708-d82c-4b10-802f-ae6ad3916feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###5. Group & Aggregate (Summarization)\n",
    "Source Files: DF of logistics_source1 and logistics_source2\n",
    "\n",
    "Scenario: Regional staffing analysis.\n",
    "Action: Group by hub_location and Count the number of staff.\n",
    "Scenario: Fleet capacity analysis.\n",
    "Action: Group by vehicle_type and Sum the shipment_weight_kg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea040da4-9110-4678-8c5c-5fa84b67de3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Scenario: Regional staffing analysis.\n",
    "#Action: Group by hub_location and Count the number of staff.\n",
    "\n",
    "merge_df_grp_agg=merge_df4.groupBy(\"hub_location\").count()\n",
    "display(merge_df_grp_agg)\n",
    "\n",
    "#5. Join (Combination)\n",
    "#Scenario: Fleet capacity analysis.\n",
    "#Action: Group by vehicle_type and Sum the shipment_weight_kg.\n",
    "\n",
    "predfjson2_group_agg=predfjson2.groupBy(\"vehicle_type\").agg(sum(\"shipment_weight_kg\")).alias(\"shipment_weight_kg\")\n",
    "display(predfjson2_group_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd0767ed-39d8-4b4f-be2f-b8f5f0884b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####6. Sorting (Ordering)\n",
    "Source File: DF of json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "939ef1eb-dbba-4bda-a886-d6cc402cb3d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Scenario: Prioritize the most expensive shipments.\n",
    "#Action: Sort by shipment_cost in Descending order.\n",
    "predfjson2_order=predfjson2.orderBy(col(\"shipment_cost\").desc())\n",
    "display(predfjson2_order)\n",
    "\n",
    "#Scenario: Organize daily dispatch schedule.\n",
    "#Action: Sort by shipment_date (Ascending) then priority_flag (Descending).\n",
    "predfjson2_order_sort=predfjson2_order.orderBy(col(\"shipment_date\").asc())\n",
    "display(predfjson2_order_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5135c086-a7e7-411b-825e-421c176c0101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####7. Limit (Top-N Analysis)\n",
    "Source File: DF of json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78ccebc3-a364-4283-84f5-3c1cd04365ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Scenario: Dashboard snapshot of critical delays.\n",
    "#Action: Filter for 'DELAYED', Sort by Cost, and Limit to top 10 rows.\n",
    "predfjson2_order_sort_limit=predfjson2_order_sort.filter(col(\"shipment_status\")=='DELAYED').orderBy(col(\"shipment_cost\").desc()).limit(10)\n",
    "display(predfjson2_order_sort_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a73291b-7a12-4e3a-b40d-f2de9a38eea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Data Wrangling - Transformation & Analytics\n",
    "*Combining, modeling, and analyzing data to answer complex business questions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14966c6c-9f02-4da8-899b-ae001aeba464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Joins\n",
    "#######Source Files:\n",
    "#######Left Side (staff_df):\n",
    "#######DF of logistics_source1 & logistics_source2\n",
    "#######Right Side (shipments_df):\n",
    "#######DF of logistics_shipment_detail_3000.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49d4b097-8c45-45f5-b5f2-187f08a0b675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(merge_df4)\n",
    "staff_df=merge_df4\n",
    "staff_df=staff_df.filter(col(\"shipment_id\") !=\"ten\")\n",
    "display(staff_df)\n",
    "shipments_df=jsondf11\n",
    "display(jsondf11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1c0e139-8873-425a-80bc-8f7de1c6637c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1 Frequently Used Simple Joins (Inner, Left)\n",
    "#Inner Join (Performance Analysis):\n",
    "#Scenario: We only want to analyze completed work. Connect Staff to the Shipments they handled.\n",
    "#Action: Join staff_df and shipments_df on shipment_id.\n",
    "#Result: Returns only rows where a staff member is assigned to a valid shipment.\n",
    "innerdf=staff_df.join(shipments_df,on='shipment_id',how='inner')\n",
    "display(innerdf)\n",
    "#Left Join (Idle Resource check):\n",
    "#Scenario: Find out which staff members are currently idle (not assigned to any shipment).\n",
    "#Action: Join staff_df (Left) with shipments_df (Right) on shipment_id. Filter where shipments_df.shipment_id is NULL.\n",
    "leftdf=staff_df.join(shipments_df,on='shipment_id',how='left')\n",
    "leftdf=leftdf.filter(col(\"shipment_id\").isNull())\n",
    "#leftdf=leftdf.filter(col(\"shipment_id\").isNotNull())\n",
    "display(leftdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a853cfb-1ac7-4caf-8b5b-017c075cbed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **1.2 Infrequent Simple Joins (Self, Right, Full, Cartesian)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf873c72-7c17-40b9-b956-909fff4e3370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Self Join (Peer Finding):\n",
    "#Scenario: Find all pairs of employees working in the same hub_location.\n",
    "#Action: Join staff_df to itself on hub_location, filtering where staff_id_A != staff_id_B.\n",
    "selfdf=staff_df.alias(\"staff_id_A\").join(staff_df.alias(\"staff_id_B\"),on='hub_location',how='inner').filter(col(\"staff_id_A.shipment_id\") != col(\"staff_id_B.shipment_id\"))\n",
    "#display(selfdf)\n",
    "#Right Join (Orphan Data Check):\n",
    "#Scenario: Identify shipments in the system that have no valid driver assigned (Data Integrity Issue).\n",
    "#Action: Join staff_df (Left) with shipments_df (Right). Focus on NULLs on the left side.\n",
    "\n",
    "rightdf=staff_df.join(shipments_df,on='shipment_id',how='right')\n",
    "rightdf=rightdf.filter(col(\"first_name\").isNull())\n",
    "\n",
    "#display(rightdf)\n",
    "#2 Frequently Used Complex Joins (Outer, Cross)\n",
    "#Full Outer Join (Reconciliation):\n",
    "#Scenario: A complete audit to find both idle drivers AND unassigned shipments in one view.\n",
    "#Action: Perform a Full Outer Join on shipment_id.\n",
    "\n",
    "fulldf=staff_df.join(shipments_df,on='shipment_id',how='outer')\n",
    "display(fulldf)\n",
    "\n",
    "#Cartesian/Cross Join (Capacity Planning):\n",
    "#Scenario: Generate a schedule of every possible driver assignment to every pending shipment to run an optimization algorithm.\n",
    "#Action: Cross Join drivers_df and pending_shipments_df.\n",
    "\n",
    "crossdf=staff_df.join(shipments_df,how='cross')\n",
    "display(crossdf)\n",
    "#3 Frequently Used Joins with Conditions (Left Semi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee60d18d-ea6e-4e82-92be-1d19c6d1ee27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1.3 Advanced Joins (Semi and Anti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "611f4b7d-4636-4e12-ba8e-661013d8d445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Left Semi Join (Existence Check):\n",
    "#Scenario: \"Show me the details of Drivers who have at least one shipment.\" (Standard filtering).\n",
    "#Action: staff_df.join(shipments_df, \"shipment_id\", \"left_semi\").\n",
    "\n",
    "leftsemidf=staff_df.join(shipments_df,on='shipment_id',how='left_semi')\n",
    "display(leftsemidf)\n",
    "\n",
    "#Benefit: Performance optimization; it stops scanning the right table once a match is found.\n",
    "#Left Anti Join (Negation Check):\n",
    "#Scenario: \"Show me the details of Drivers who have never touched a shipment.\"\n",
    "#Action: staff_df.join(shipments_df, \"shipment_id\", \"left_anti\").\n",
    "\n",
    "leftantidf=staff_df.join(shipments_df,on='shipment_id',how='left_anti')\n",
    "display(leftantidf)\n",
    "#4 Frequently Used Joins with Conditions (Left Anti)\n",
    "#Left Anti Join (Negation Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "248c67fa-b3d3-475b-b753-50b3c09fd0d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF)\n",
    "\n",
    "#Scenario: Validation. Check if the hub_location in the staff file exists in the corporate Master_City_List.\n",
    "#Action: Compare values against a reference list.\n",
    "master_cities = [\n",
    "    (\"Bangalore\",),\n",
    "    (\"Chennai\",),\n",
    "    (\"Mumbai\",),\n",
    "    (\"Delhi\",),\n",
    "    (\"Pune\",),\n",
    "    (\"Hyderabad\",),\n",
    "    (\"Jaipur\",),\n",
    "    (\"Kochi\",),\n",
    "    (\"Indore\",)\n",
    "]\n",
    "\n",
    "master_city_df = spark.createDataFrame(master_cities, [\"hub_location\"])\n",
    "display(master_city_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0fd31f1-41e9-4eae-b394-8b435c43d286",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "invalid_hubs_df = staff_df.join(\n",
    "    master_city_df,\n",
    "    on=\"hub_location\",\n",
    "    how=\"left_semi\"\n",
    ")\n",
    "\n",
    "display(invalid_hubs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbcbb1b1-31f1-4733-a9f2-7dcea53e8ff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "master_df=spark.read.csv(\"/Volumes/we47catalog1/we47db1/we47volume1/Master_City_List.csv\",header=True,inferSchema=True)\n",
    "display(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95869c7f-4e78-422f-8e45-ab4d803da172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF) and Master_City_List.csv\n",
    "\n",
    "#Scenario: Validation. Check if the hub_location in the staff file exists in the dataframe of corporate Master_City_List.csv.\n",
    "#Action: Compare values against this Master_City_List list.\n",
    "lookdf_semi=staff_df.join(master_df,on=col('hub_location')==col('city_name'),how='left_semi')\n",
    "display(lookdf_semi)\n",
    "\n",
    "##Scenario: Validation. Check if the hub_location in the staff file not exists in the dataframe of corporate Master_City_List.csv.\n",
    "\n",
    "lookdf_anti=staff_df.join(master_df,on=col('hub_location')==col('city_name'),how='left_anti')\n",
    "display(lookdf_anti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5afafabc-a89a-4607-b9fe-63f6aa8fc93f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **3. Lookup & Enrichment**<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF) and Master_City_List.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4264662e-face-4b96-ab92-8c36522d156c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF) and Master_City_List.csv dataframe\n",
    "\n",
    "#Scenario: Geo-Tagging.\n",
    "#Action: Lookup hub_location (eg. \"Pune\") in a Master Latitude/Longitude Master_City_List.csv dataframe and enrich our logistics_source (merged dataframe) by adding lat and long columns for map plotting.\n",
    "\n",
    "\n",
    "staff_df_look=staff_df.join(master_df,on=col(\"hub_location\") == col(\"city_name\"),how='left')\n",
    "display(staff_df_look)\n",
    "\n",
    "staff_df_look_enrich=staff_df.join(master_df,on=col(\"hub_location\") == col(\"city_name\"),how='inner')\n",
    "display(staff_df_look_enrich)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61ea2da1-aae8-4a93-b31d-e584caf1c98c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **4. Schema Modeling (Denormalization)**<br>\n",
    "Source Files: DF of All 3 Files (logistics_source1, logistics_source2, logistics_shipment_detail_3000.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1896da8-5389-4db6-abf8-dc2778add973",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fix join error in gold layer creation"
    }
   },
   "outputs": [],
   "source": [
    "#Scenario: Creating a \"Gold Layer\" Table for PowerBI/Tableau.\n",
    "#Action: Flatten the Star Schema. Join Staff, Shipments, and Vehicle_Master into one wide table (wide_shipment_history) so analysts don't have to perform joins during reporting.\n",
    "\n",
    "#applying left outer to ensure all customer data is present\n",
    "star_df=staff_df.join(shipments_df,on='shipment_id',how='left')\n",
    "#display(star_df)\n",
    "\n",
    "star_df1=star_df.join(master_df, star_df.hub_location == master_df.city_name, how='inner')\n",
    "display(star_df1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3ac7879-bb6b-4f20-ba41-a185322a2af5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###5. Windowing (Ranking & Trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81803d6d-c040-4451-a56a-2c76f6f79283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Source Files:\n",
    "#DF of logistics_source2: Provides hub_location (Partition Key).\n",
    "#logistics_shipment_detail_3000.json: Provides shipment_cost (Ordering Key)\n",
    "from pyspark.sql.functions import row_number,col\n",
    "from pyspark.sql.window import Window\n",
    "window_df1=spark.read.csv(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_source2\",header=\"True\",inferSchema=\"True\")\n",
    "window_df1=window_df1.filter(col(\"shipment_id\") !=\"ten\")\n",
    "#display(window_df1)\n",
    "window_df1.printSchema()\n",
    "window_df2=spark.read.json(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_shipment_detail_3000.json\",multiLine=True)\n",
    "#display(window_df2)\n",
    "#window_df2.printSchema()\n",
    "\n",
    "window_df3=window_df1.join(window_df2,on='shipment_id',how='inner')\n",
    "#display(window_df3)\n",
    "###`row_number()\n",
    "row_df4=window_df3.withColumn(\"row_number_column\",row_number().over(Window.partitionBy(col(\"hub_location\")).orderBy(col(\"shipment_cost\").desc())))\n",
    "row_df4=row_df4.filter(col(\"row_number_column\")<=3)\n",
    "display(row_df4)\n",
    "\n",
    "####dence_rank\n",
    "dence_df5=window_df3.withColumn(\"dence_rank_column\", dense_rank().over(Window.partitionBy(col(\"hub_location\")).orderBy(col(\"shipment_cost\").desc())))\n",
    "dence_df5=dence_df5.filter(col(\"dence_rank_column\")<=3)\n",
    "display(dence_df5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4fe49cc-7299-4b59-958e-038a732aaa96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###6. Analytical Functions (Lead/Lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "943f7f22-7651-4629-9528-d42709c84b08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, lag, col, try_to_date\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_df = spark.read.json(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_shipment_detail_3000.json\", multiLine=True)\n",
    "display(window_df)\n",
    "\n",
    "# Use try_to_date to convert shipment_date to DATE\n",
    "window_df = window_df.withColumn(\"shipment_date\", try_to_date(col(\"shipment_date\"), \"yy-MM-dd\"))\n",
    "\n",
    "lagdf = window_df.withColumn(\n",
    "    \"prev_shipment_date\",\n",
    "    lag(\"shipment_date\", 1).over(Window.partitionBy(col(\"vehicle_type\")).orderBy(col(\"shipment_date\")))\n",
    ")\n",
    "lagdf = lagdf.withColumn(\n",
    "    \"days_elapsed\",\n",
    "    datediff(col(\"shipment_date\"), col(\"prev_shipment_date\"))\n",
    ")\n",
    "lagdf = lagdf.drop(\"prev_shipment_date\")\n",
    "display(lagdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c044401-045f-4612-b916-8b62fcf18266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **7. Set Operations**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ccae5c9-4507-4f2f-909f-afeffc8ae5c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Union: Combining Source1 (Legacy) and Source2 (Modern) into one dataset (Already done in Active Munging).\n",
    "from pyspark.sql.functions import lit,initcap\n",
    "\n",
    "struct1=\"shipment_id string ,first_name string ,last_name string ,age string ,role string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_source1\",header=True,inferSchema=True).withColumn(\"sourcesystem1\",lit(\"system1\"))\n",
    "rawdf1_u1=rawdf1.select(\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\")\n",
    "#display(rawdf1)\n",
    "rawdf2=spark.read.csv(\"/Volumes/we47catalog1/we47db1/we47volume1/logistics_source2\",header=True,inferSchema=True).withColumn(\"sourcesystem2\",lit(\"system2\"))\n",
    "rawdf2_u2=rawdf2.select(\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\")\n",
    "#display(rawdf2)\n",
    "\n",
    "uniondf=rawdf1_u1.union(rawdf2_u2)    \n",
    "display(uniondf)\n",
    "\n",
    "uniondf1=rawdf1_u1.unionAll(rawdf2_u2).distinct()\n",
    "display(uniondf)\n",
    "\n",
    "#Intersect: Identifying Staff IDs that appear in both Source 1 and Source 2 (Duplicate/Migration Check).\n",
    "\n",
    "common_df=rawdf1_u1.intersect(rawdf2_u2).distinct()\n",
    "display(common_df)\n",
    "\n",
    "#Except (Difference): Identifying Staff IDs present in Source 2 but missing from Source 1 (New Hires).\n",
    "#returns df1-df2 data (excessive df1 data)\n",
    "uniondf2=rawdf1_u1.subtract(rawdf2_u2)\n",
    "display(uniondf2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd86a2cf-238c-4033-99a7-04d3aa38e79d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###8. Grouping & Aggregations (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd1082bd-6584-4f9e-a75a-5f6c538b046d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fix ambiguous vehicle_type reference"
    }
   },
   "outputs": [],
   "source": [
    "#Source Files:\n",
    "#DF of logistics_source2: Provides hub_location and vehicle_type (Grouping Dimensions).\n",
    "#DF of logistics_shipment_detail_3000.json: Provides shipment_cost (Aggregation Metric).\n",
    "#Scenario: The CFO wants a subtotal report at multiple levels:\n",
    "#Total Cost by Hub.\n",
    "#Total Cost by Hub AND Vehicle Type.\n",
    "#Grand Total.\n",
    "\n",
    "display(jsondf4)\n",
    "rawdf2=rawdf2.filter(col(\"shipment_id\") !=\"ten\")\n",
    "display(rawdf2)\n",
    "\n",
    "mergedf=rawdf2.alias(\"raw\").join(jsondf4.alias(\"json\"),on=col(\"raw.shipment_id\")==col(\"json.shipment_id\"), how=\"inner\")\n",
    "display(mergedf)\n",
    "\n",
    "agg_sum=mergedf.cube(mergedf[\"raw.hub_location\"], mergedf[\"raw.vehicle_type\"]).agg(sum(mergedf[\"json.shipment_cost\"]).alias(\"total_cost\"))\n",
    "display(agg_sum)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eff3ace7-6c7f-4773-b0f5-ff41429b9cb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Data Persistance (LOAD)-> Data Publishing & Consumption<br>\n",
    "\n",
    "Store the inner joined, lookup and enrichment, Schema Modeling, windowing, analytical functions, set operations, grouping and aggregation data into the delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6bebbd5-f7c1-46d4-9a56-72479e6280bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Schema Modeling data stored into delta lake using spark dataframe\n",
    "\n",
    "merge_df6.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"we47catalog1.we47db1.schema_model_table\")\n",
    "##same file store into as parquet file into volume of delta lake:\n",
    "merge_df6.write.mode(\"overwrite\").format(\"parquet\").save(\"/Volumes/we47catalog1/we47db1/we47volume1/logistic_data_model\")\n",
    "\n",
    "##lookup data stored into delta lake using spark dataframe\n",
    "\n",
    "staff_df_look.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"we47catalog1.we47db1.staff_lookup\")\n",
    "##same file store into as parquet file into volume of delta lake:\n",
    "staff_df_look.write.format(\"parquet\").mode(\"overwrite\").save(\"/Volumes/we47catalog1/we47db1/we47volume1/logistic_data_model/staff_lookup\")\n",
    "\n",
    "##Enrich data stored into delta lake using spark dataframe\n",
    "staff_df_look_enrich.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"we47catalog1.we47db1.staff_lookup_enrich\")\n",
    "staff_df_look_enrich.write.mode(\"overwrite\").format(\"parquet\").save(\"/Volumes/we47catalog1/we47db1/we47volume1/logistic_data_model/staff_lookup_enrich\")\n",
    "\n",
    "#grouping and aggregation data:\n",
    "agg_sum.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"we47catalog1.we47db1.agg_sum_table\")\n",
    "agg_sum.write.format(\"parquet\").mode(\"overwrite\").save(\"/Volumes/we47catalog1/we47db1/we47volume1/logistic_data_model/group_agg\")\n",
    "\n",
    "#set operations\n",
    "uniondf2.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"we47catalog1.we47db1.uniondf2_table\")\n",
    "uniondf2.write.mode(\"overwrite\").format(\"parquet\").save(\"/Volumes/we47catalog1/we47db1/we47volume1/logistic_data_model/uniondf2_table_data\")\n",
    "\n",
    "common_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"we47catalog1.we47db1.common_df_table\")\n",
    "common_df.write.mode(\"overwrite\").format(\"delta\").save(\"/Volumes/we47catalog1/we47db1/we47volume1/logistic_data_model/common_df_table_data\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.logistics_usecase_BB_DSL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
